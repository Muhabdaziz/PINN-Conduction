{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLomUAvByF54"
   },
   "source": [
    "# **Preparation**\n",
    "\n",
    "``note: jalankan semua code ini & pastikan pakai GPU biar cepet``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646374646174,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "5mt2UaM0mx9J"
   },
   "outputs": [],
   "source": [
    "# Benerin / cek data test apakah include untuk training ato nggak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7ReT07SyIqg"
   },
   "source": [
    "## **Import Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82577,
     "status": "ok",
     "timestamp": 1646374729418,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "LJbmWzPJyMUZ",
    "outputId": "53fb2245-83a6-4ada-96aa-db371a4294c6"
   },
   "outputs": [],
   "source": [
    "# Install\n",
    "!pip install PyDOE\n",
    "!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3279,
     "status": "ok",
     "timestamp": 1646374732692,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "-jTowqixnhvL"
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import timeit\n",
    "import math as m\n",
    "import scipy.interpolate\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from pyDOE import lhs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1646374732692,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "KgZ9q0wcyOYd"
   },
   "outputs": [],
   "source": [
    "#! sudo apt-get install texlive-latex-recommended \n",
    "#! sudo apt install texlive-latex-extra\n",
    "#! sudo apt install dvipng\n",
    "#! sudo apt install cm-super\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#from matplotlib import rc\n",
    "#rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "#rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-59d6G_T3U7M"
   },
   "source": [
    "## **Class & Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jxmBZup8di5"
   },
   "source": [
    "### **PINN Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4126,
     "status": "ok",
     "timestamp": 1646374736815,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "ETX5B15cY8aN"
   },
   "outputs": [],
   "source": [
    "class PinnVP:\n",
    "\n",
    "    def __init__(self, data, params, bc_type, problem_type, adaptive_model=False, improved_model=False, \n",
    "                 exist_model=False, mini_batches=False, file_dir=\"\"):\n",
    "        # Initialize & Unpack Input data\n",
    "        self.adaptive_model = adaptive_model\n",
    "        self.improved_model = improved_model\n",
    "        self.mini_batches = mini_batches\n",
    "        self.unpack(data, params)\n",
    "        self.initialize_variables()\n",
    "        self.bc_type = bc_type \n",
    "        self.problem_type = problem_type\n",
    "\n",
    "        # Initialize Neural Network Computational Graph\n",
    "        # Weight & biases\n",
    "        if exist_model:\n",
    "            print(\"Loading NN parameters ...\")\n",
    "            self.weights, self.biases = self.load_model(file_dir)\n",
    "        else:\n",
    "            self.weights, self.biases = self.initialize_network()\n",
    "\n",
    "        # Initialize encoder weights & biases\n",
    "        if self.improved_model == improved_model:\n",
    "            self.encoder_weights_1 = self.xavier_init([2, self.layers[1]])\n",
    "            self.encoder_biases_1 = self.xavier_init([1, self.layers[1]])\n",
    "\n",
    "            self.encoder_weights_2 = self.xavier_init([2, self.layers[1]])\n",
    "            self.encoder_biases_2 = self.xavier_init([1, self.layers[1]])\n",
    "\n",
    "        # Placeholder & Graph\n",
    "        # Placeholder (where we put input)\n",
    "        self.initialize_placeholders()\n",
    "\n",
    "        # Computational graph of Physics-Informed\n",
    "        self.graph_network()\n",
    "\n",
    "        # Computational graph of loss\n",
    "        self.graph_loss()\n",
    "\n",
    "        # Optimizers\n",
    "        self.initialize_optimizers()\n",
    "\n",
    "        # Initialize adaptive weights\n",
    "        if self.adaptive_model:\n",
    "            self.initialize_adaptive_weights()\n",
    "\n",
    "        # Session\n",
    "        self.initialize_session()\n",
    "\n",
    "    def callback_inverse(self, loss_test, loss_total, loss_collo, loss_meu, loss_init, lambda_1):\n",
    "        self.count += 1\n",
    "        self.loss_test_log.append(loss_test)\n",
    "        self.loss_total_log.append(loss_total)\n",
    "        self.loss_collo_log.append(loss_collo)\n",
    "        self.loss_meu_log.append(loss_meu)\n",
    "        self.loss_init_log.append(loss_init)\n",
    "        self.lambda_1_log.append(lambda_1)\n",
    "\n",
    "        if self.count % self.verboses_newton == 0:    \n",
    "           print(\"iter: %d, Loss Test: %.4e, Loss Total: %.4e, Loss Collo: %.4e, Loss Measurement: %.4e, Loss Init: %.4e, lambda_1: %.3f\" %\n",
    "                 (self.count, loss_test, loss_total, loss_collo, loss_meu, loss_init, lambda_1[0]))\n",
    "    \n",
    "    def callback(self, loss_test, loss_total, loss_collo, loss_meu, loss_init):\n",
    "        self.count += 1\n",
    "        self.loss_test_log.append(loss_test)\n",
    "        self.loss_total_log.append(loss_total)\n",
    "        self.loss_collo_log.append(loss_collo)\n",
    "        self.loss_meu_log.append(loss_meu)\n",
    "        self.loss_init_log.append(loss_init)\n",
    "        \n",
    "        if self.count % self.verboses_newton == 0:    \n",
    "           print(\"iter: %d, Loss Test: %.4e, Loss Total: %.4e, Loss Collo: %.4e, Loss Measurement: %.4e, Loss Init: %.4e\" %\n",
    "                 (self.count, loss_test, loss_total, loss_collo, loss_meu, loss_init))\n",
    "\n",
    "        \n",
    "\n",
    "    def normalize_input_data(self):\n",
    "        # Normalize data\n",
    "        x_c = self.normalize_data(data=self.x_c, axis=\"x\")\n",
    "        # x_c = self.x_c\n",
    "        y_c = self.normalize_data(data=self.y_c, axis=\"y\")\n",
    "#         x_inlet = self.normalize_data(data=self.x_inlet, axis=\"x\")\n",
    "        # x_inlet = self.x_inlet\n",
    "#         y_inlet = self.normalize_data(data=self.y_inlet, axis=\"y\")\n",
    "        x_meu = self.normalize_data(data=self.x_meu, axis=\"x\")\n",
    "        # x_outlet = self.x_outlet\n",
    "        y_meu = self.normalize_data(data=self.y_meu, axis=\"y\")\n",
    "        x_wall = self.normalize_data(data=self.x_wall, axis=\"x\")\n",
    "        # x_wall = self.x_wall\n",
    "        y_wall = self.normalize_data(data=self.y_wall, axis=\"y\")\n",
    "        x_test = self.normalize_data(data=self.x_test, axis=\"x\")\n",
    "        # x_test = self.x_test\n",
    "        y_test = self.normalize_data(data=self.y_test, axis=\"y\")\n",
    "\n",
    "        return x_c, y_c, x_meu, y_meu, x_wall, y_wall, x_test, y_test\n",
    "\n",
    "    def fetch_minibatch(self, x, y):\n",
    "        mini_batches = []\n",
    "        batch_size = self.batch_size\n",
    "        data = np.hstack((x, y))\n",
    "        np.random.shuffle(data)\n",
    "        n_mini_batches = data.shape[0] // batch_size\n",
    "\n",
    "        i = 0\n",
    "        for i in range(n_mini_batches):\n",
    "            mini_batch = data[i*batch_size:(i+1)*batch_size, :]\n",
    "            x_mini = mini_batch[:, :-1]\n",
    "            y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
    "            mini_batches.append(np.hstack((x_mini, y_mini)))\n",
    "\n",
    "        return mini_batches\n",
    "\n",
    "    def create_input_dict(self):\n",
    "        # Normalize data\n",
    "        x_c, y_c, x_inlet, y_inlet, x_outlet, y_outlet, x_wall, y_wall, x_test, y_test = self.normalize_input_data()\n",
    "\n",
    "        # Mini Batches\n",
    "        if self.mini_batches:\n",
    "            tf_dict_batches = []\n",
    "            mini_batches_input = self.fetch_minibatch(x=x_c, y=y_c)\n",
    "\n",
    "            for mini_batch in mini_batches_input:\n",
    "                # unpack\n",
    "                x_mini = mini_batch[:, 0]\n",
    "                y_mini = mini_batch[:, 1]\n",
    "\n",
    "                n = len(x_mini)\n",
    "\n",
    "                x_mini = x_mini.reshape(n, 1)\n",
    "                y_mini = y_mini.reshape(n, 1)\n",
    "\n",
    "                if self.adaptive_model:\n",
    "                    tf_dict = {self.x_c_tf: x_mini, self.y_c_tf: y_mini,\n",
    "                               self.x_inlet_tf: x_inlet, self.y_inlet_tf: y_inlet,              # inlet data (pts)\n",
    "                               self.u_inlet_tf: self.u_inlet, self.v_inlet_tf: self.v_inlet,    # inlet data (velocity)\n",
    "                               self.x_outlet_tf: x_outlet, self.y_outlet_tf: y_outlet,          # outlet data\n",
    "                               self.x_wall_tf: x_wall, self.y_wall_tf: y_wall,                  # wall data\n",
    "                               self.u_wall_tf: self.u_wall, self.v_wall_tf: self.v_wall,        \n",
    "                               self.x_test_tf: x_test, self.y_test_tf: y_test,                  # test data\n",
    "                               self.u_test_tf: self.u_test,\n",
    "                               self.coef_bc_tf: self.coef_bc_val}\n",
    "                else:\n",
    "                    tf_dict = {self.x_c_tf: x_mini, self.y_c_tf: y_mini,\n",
    "                               self.x_inlet_tf: x_inlet, self.y_inlet_tf: y_inlet,              # inlet data (pts)\n",
    "                               self.u_inlet_tf: self.u_inlet, self.v_inlet_tf: self.v_inlet,    # inlet data (velocity)\n",
    "                               self.x_outlet_tf: x_outlet, self.y_outlet_tf: y_outlet,          # outlet data\n",
    "                               self.x_wall_tf: x_wall, self.y_wall_tf: y_wall,                  # wall data\n",
    "                               self.u_wall_tf: self.u_wall, self.v_wall_tf: self.v_wall,        \n",
    "                               self.x_test_tf: x_test, self.y_test_tf: y_test,                  # test data\n",
    "                               self.u_test_tf: self.u_test}\n",
    "\n",
    "                # add to tf dict batches\n",
    "                tf_dict_batches.append(tf_dict)\n",
    "\n",
    "            return tf_dict_batches\n",
    "\n",
    "        else:\n",
    "            if self.adaptive_model:\n",
    "                tf_dict = {self.x_c_tf: x_c, self.y_c_tf: y_c,\n",
    "                           self.x_inlet_tf: x_inlet, self.y_inlet_tf: y_inlet,              # inlet data (pts)\n",
    "                           self.u_inlet_tf: self.u_inlet, \n",
    "                          #  self.v_inlet_tf: self.v_inlet,    # inlet data (velocity)\n",
    "                           self.x_outlet_tf: x_outlet, self.y_outlet_tf: y_outlet, \n",
    "                           self.u_outlet_tf: self.u_outlet, # outlet data\n",
    "                           self.x_wall_tf: x_wall, self.y_wall_tf: y_wall,                  # wall data\n",
    "                           self.u_wall_tf: self.u_wall, \n",
    "                          #  self.v_wall_tf: self.v_wall,        \n",
    "                           self.x_test_tf: x_test, self.y_test_tf: y_test,                  # test data\n",
    "                           self.u_test_tf: self.u_test,\n",
    "                           self.coef_bc_tf: self.coef_bc_val}\n",
    "            else:\n",
    "                tf_dict = {self.x_c_tf: x_c, self.y_c_tf: y_c,\n",
    "                            self.x_inlet_tf: x_inlet, self.y_inlet_tf: y_inlet,              # inlet data (pts)\n",
    "                            self.u_inlet_tf: self.u_inlet, \n",
    "                            # self.v_inlet_tf: self.v_inlet,    # inlet data (velocity)\n",
    "                            self.x_outlet_tf: x_outlet, self.y_outlet_tf: y_outlet,          # outlet data\n",
    "                            self.u_outlet_tf: self.u_outlet,\n",
    "                            self.x_wall_tf: x_wall, self.y_wall_tf: y_wall,                  # wall data\n",
    "                            self.u_wall_tf: self.u_wall, \n",
    "                            # self.v_wall_tf: self.v_wall,        \n",
    "                            self.x_test_tf: x_test, self.y_test_tf: y_test,                  # test data\n",
    "                            # self.x_tf: self.x,\n",
    "                            self.u_test_tf: self.u_test}\n",
    "                            # self.rho_tf: self.RHO, self.cp_tf: self.cp, self.k_tf: self.k}\n",
    "\n",
    "            return tf_dict\n",
    "            \n",
    "    def fit_adam(self):\n",
    "        # Change flag\n",
    "        self.adam_started = True\n",
    "\n",
    "        # Start iteration\n",
    "        print(\"START TRAINING ADAM\")\n",
    "        print(\"-------------------------------------------------------------\")\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # Create dictionary\n",
    "            if self.mini_batches:\n",
    "                tf_dicts = self.create_input_dict()\n",
    "            else:\n",
    "                tf_dict = self.create_input_dict()\n",
    "\n",
    "            # Run Adam optimizer\n",
    "            if self.mini_batches:\n",
    "                for tf_dict in tf_dicts:\n",
    "                    self.sess.run(self.train_op_adam, tf_dict)\n",
    "            else:\n",
    "                self.sess.run(self.train_op_adam, tf_dict)\n",
    "\n",
    "            # Adaptive modeling\n",
    "            if self.adaptive_model:\n",
    "                if epoch % self.verboses_adaptive == 0:\n",
    "                    coef_bc_val = self.sess.run(self.coef_bc, tf_dict)\n",
    "                    self.coef_bc_val = coef_bc_val*(1.0-self.beta) + self.beta*self.coef_bc_val\n",
    "                    self.coef_bc_log.append(self.coef_bc_val)\n",
    "\n",
    "            # Print\n",
    "            if epoch % self.verboses_adam == 0:\n",
    "                # Calculate loss\n",
    "                loss_total_ = self.sess.run(self.loss_total, tf_dict)\n",
    "                loss_collo_ = self.sess.run(self.loss_collo, tf_dict)\n",
    "                loss_bound_ = self.sess.run(self.loss_bound, tf_dict)\n",
    "                loss_init_ = self.sess.run(self.loss_init, tf_dict)\n",
    "                loss_test_ = self.sess.run(self.loss_test, tf_dict)\n",
    "\n",
    "                if self.bc_type == 'Neumann':\n",
    "                   grad_in = self.sess.run(self.grad_x_inlet,tf_dict)\n",
    "                   grad_out = self.sess.run(self.grad_x_outlet,tf_dict)\n",
    "\n",
    "                # Save loss\n",
    "                self.loss_total_log.append(loss_total_)\n",
    "                self.loss_collo_log.append(loss_collo_)\n",
    "                self.loss_bound_log.append(loss_bound_)\n",
    "                self.loss_init_log.append(loss_init_)\n",
    "                self.loss_test_log.append(loss_test_)\n",
    "\n",
    "                if self.problem_type == \"Inverse\":\n",
    "                   lambda_1_value = self.sess.run(self.lambda_1)\n",
    "                   self.lambda_1_log.append(lambda_1_value)\n",
    "\n",
    "                   elapsed = timeit.default_timer() - start_time\n",
    "                   print(\"iter: %d, Loss Test: %.4e, Loss Total: %.4e, Loss Collo: %.4e, Loss Boundary: %.4e, Loss Init: %.4e, lambda_1: %.3f, time: %.2fs\" %\n",
    "                         (epoch+self.last_adam_iter, loss_test_, loss_total_, loss_collo_, loss_bound_, loss_init_, lambda_1_value[0], elapsed))\n",
    "                else:\n",
    "                   elapsed = timeit.default_timer() - start_time\n",
    "                   print(\"iter: %d, Loss Test: %.4e, Loss Total: %.4e, Loss Collo: %.4e, Loss Boundary: %.4e, Loss Init: %.4e, time: %.2fs\" %\n",
    "                         (epoch+self.last_adam_iter, loss_test_, loss_total_, loss_collo_, loss_bound_, loss_init_, elapsed))\n",
    "\n",
    "                \n",
    "                if self.bc_type == \"Neumann\":\n",
    "                  print(np.concatenate((grad_in, grad_out), axis=1))\n",
    "                \n",
    "                if self.adaptive_model:\n",
    "                    print(\"boundary weights coefficients: {:.4f}\".format(self.coef_bc_val))\n",
    "\n",
    "                start_time = timeit.default_timer()\n",
    "\n",
    "            if epoch % self.saver == 0:\n",
    "                self.save_model(f\"model-{epoch}.pickle\")\n",
    "                self.save_loss(f\"model-loss-{epoch}.csv\")\n",
    "                print(\"model & loss saved\")\n",
    "\n",
    "        self.last_adam_iter += self.epochs\n",
    "        self.lr_init = self.coef_bc_val\n",
    "\n",
    "    def fit_newton(self):\n",
    "        # Change flag\n",
    "        self.newton_started = True\n",
    "        self.count += self.last_adam_iter\n",
    "\n",
    "        # Normalize data\n",
    "        (x_c, y_c, \n",
    "#          x_inlet, y_inlet, \n",
    "         x_meu, y_meu, \n",
    "         x_wall, y_wall, \n",
    "         x_test, y_test) = self.normalize_input_data()\n",
    "\n",
    "        # Create dictionary\n",
    "        tf_dict = {self.x_c_tf: x_c, self.y_c_tf: y_c,                              # collocation data\n",
    "#                    self.x_inlet_tf: x_inlet, self.y_inlet_tf: y_inlet,              # inlet data (pts)\n",
    "#                    self.u_inlet_tf: self.u_inlet, \n",
    "                  #  self.v_inlet_tf: self.v_inlet,    # inlet data (velocity)\n",
    "                   self.x_meu_tf: x_meu, self.y_meu_tf: y_meu,          # outlet data\n",
    "                   self.u_meu_tf: self.u_meu,\n",
    "                   self.x_wall_tf: x_wall, self.y_wall_tf: y_wall,                  # wall data\n",
    "                   self.u_wall_tf: self.u_wall, \n",
    "                  #  self.v_wall_tf: self.v_wall,        \n",
    "                   self.x_test_tf: x_test, self.y_test_tf: y_test,                  # test data\n",
    "                  #  self.x_tf:self.x,\n",
    "                   self.u_test_tf:self.u_test, self.coef_bc_tf:self.coef_bc_val}\n",
    "                  #  self.rho_tf: self.RHO, self.cp_tf: self.cp, self.k_tf: self.k}\n",
    "\n",
    "        # Optimize\n",
    "        if self.problem_type == \"Inverse\":\n",
    "           self.train_op_newton.minimize(self.sess,\n",
    "                                         feed_dict=tf_dict,\n",
    "                                         fetches=[self.loss_test, self.loss_total, \n",
    "                                                 self.loss_collo, self.loss_meu, self.loss_initial,\n",
    "                                                 self.lambda_1],\n",
    "                                         loss_callback=self.callback_inverse)\n",
    "        else:\n",
    "           self.train_op_newton.minimize(self.sess,\n",
    "                                          feed_dict=tf_dict,\n",
    "                                          fetches=[self.loss_test, self.loss_total, \n",
    "                                                  self.loss_collo, self.loss_meu, self.loss_initial],\n",
    "                                          loss_callback=self.callback)\n",
    "        \n",
    "        # Return learning rate\n",
    "        self.lr_init = self.lr_tf\n",
    "    \n",
    "    def graph_loss(self):\n",
    "        # Test\n",
    "        self.loss_test = tf.math.sqrt(tf.reduce_mean(tf.square(self.u_test_tf - self.u_test_pred))) #/tf.reduce_mean(tf.square(self.u_test_tf)))\n",
    "        # self.loss_test = tf.math.sqrt(tf.reduce_mean(tf.square(self.u_test_tf - self.u_test_pred)))\n",
    "\n",
    "        # Collocation points\n",
    "        self.loss_collo = tf.reduce_mean(tf.square(self.f_pred_u)) \\\n",
    "                          # + tf.reduce_mean(tf.square(self.f_pred_v))\n",
    "        if self.bc_type == 'Dirichlet':\n",
    "            # Boundary\n",
    "            # Inlet: u & v to dirichlet conditions\n",
    "#             self.loss_inlet = tf.reduce_mean(tf.square(self.u_inlet_pred-self.u_inlet_tf)) \n",
    "                          # + tf.reduce_mean(tf.square(self.v_inlet_pred-self.v_inlet_tf))\n",
    "        \n",
    "\n",
    "            # # Outlet: p to dirichlet conditions\n",
    "            # self.loss_outlet = tf.reduce_mean(tf.square(self.p_outlet_pred))\n",
    "            self.loss_meu = tf.reduce_mean(tf.square(self.u_meu_pred-self.u_meu_tf))\n",
    "            \n",
    "        elif self.bc_type == 'Neumann':\n",
    "            \n",
    "            \n",
    "            T_out = self.net_dnn(self.x_outlet_tf, self.y_outlet_tf)\n",
    "            T_in = self.net_dnn(self.x_inlet_tf, self.y_inlet_tf)\n",
    "            self.grad_x_outlet = tf.gradients(T_out, self.x_outlet_tf)[0] / self.sigma_x\n",
    "            self.grad_x_inlet = tf.gradients(T_in, self.x_inlet_tf)[0] / self.sigma_x\n",
    "\n",
    "            k = self.k\n",
    "            k_out = (T_out - k[0,0])/(k[1,0] - k[0,0])*(k[1,1] - k[0,1]) + k[0,1]\n",
    "            k_in = (T_in - k[0,0])/(k[1,0] - k[0,0])*(k[1,1] - k[0,1]) + k[0,1]\n",
    "\n",
    "            self.loss_inlet = tf.reduce_mean(tf.square(k_in*self.grad_x_inlet-self.u_inlet_tf)) \n",
    "            self.loss_outlet = tf.reduce_mean(tf.square(k_out*self.grad_x_outlet-self.u_outlet_tf))\n",
    "\n",
    "        # Wall: u & v to dirichlet conditions\n",
    "        self.loss_initial = tf.reduce_mean(tf.square(self.u_wall_pred-self.u_wall_tf)) \n",
    "                        #  + tf.reduce_mean(tf.square(self.v_wall_pred-self.v_wall_tf))\n",
    "        \n",
    "#         if self.adaptive_model:\n",
    "#             self.loss_bound = self.coef_bc_tf*(self.loss_inlet + self.loss_outlet)\n",
    "#             self.loss_init = self.coef_bc_tf*(self.loss_initial)\n",
    "#         else:\n",
    "#             self.loss_bound = self.coef_bc_val*(self.loss_inlet + self.loss_outlet)\n",
    "#             self.loss_init = self.coef_bc_val*self.loss_initial\n",
    "\n",
    "        # Total loss\n",
    "        self.loss_total = self.loss_collo + self.loss_meu + self.loss_initial\n",
    "\n",
    "    def graph_network(self):\n",
    "        # Test data\n",
    "        # if self.problem_type == 'Inverse':\n",
    "        #    (self.u_test_pred, self.n_test_pred) = self.net_dnn(self.x_test_tf, self.y_test_tf)\n",
    "        #    (self.u_pred, self.n_pred) = self.net_dnn(self.x_tf, self.y_tf)\n",
    "        #    (self.f_pred_u, ) = self.net_physics(self.x_c_tf, self.y_c_tf)\n",
    "        #    (self.u_inlet_pred) = self.net_dnn(self.x_inlet_tf, self.y_inlet_tf)\n",
    "        #    (self.u_outlet_pred, self.n_outlet) = self.net_dnn(self.x_outlet_tf, self.y_outlet_tf)\n",
    "        #    (self.u_wall_pred, self.n_wall_pred) = self.net_dnn(self.x_wall_tf, self.y_wall_tf)\n",
    "        # (self.u_test_pred, self.v_test_pred, self.p_test_pred) = self.net_dnn(self.x_test_tf, self.y_test_tf)\n",
    "        (self.u_test_pred) = self.net_dnn(self.x_test_tf, self.y_test_tf)\n",
    "        \n",
    "        # Predict data\n",
    "        # (self.u_pred, self.v_pred, self.p_pred) = self.net_dnn(self.x_tf, self.y_tf)\n",
    "        (self.u_pred) = self.net_dnn(self.x_tf, self.y_tf)\n",
    "\n",
    "        # Physics Training\n",
    "        # Collocation points\n",
    "        # (self.f_pred_u, self.f_pred_v) = self.net_physics(self.x_c_tf, self.y_c_tf)\n",
    "        (self.f_pred_u) = self.net_physics(self.x_c_tf, self.y_c_tf)\n",
    "\n",
    "        # Inlet: u & v\n",
    "        # (self.u_inlet_pred, self.v_inlet_pred, _) = self.net_dnn(self.x_inlet_tf, self.y_inlet_tf)\n",
    "#         (self.u_inlet_pred) = self.net_dnn(self.x_inlet_tf, self.y_inlet_tf)\n",
    "\n",
    "        # Outlet: pressure\n",
    "        # (_, _, self.p_outlet_pred) = self.net_dnn(self.x_outlet_tf, self.y_outlet_tf)\n",
    "        (self.u_meu_pred) = self.net_dnn(self.x_meu_tf, self.y_meu_tf)\n",
    "\n",
    "        # Wall: u & V\n",
    "        # (self.u_wall_pred, self.v_wall_pred, _) = self.net_dnn(self.x_wall_tf, self.y_wall_tf)\n",
    "        (self.u_wall_pred) = self.net_dnn(self.x_wall_tf, self.y_wall_tf)\n",
    "\n",
    "    def load_model(self, file_dir):\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(self.layers)\n",
    "        \n",
    "        with open(file_dir, 'rb') as f:\n",
    "            dnn_weights, dnn_biases = pickle.load(f)\n",
    "\n",
    "            # stored model mush has the same layers\n",
    "            assert num_layers == (len(dnn_weights)+1)\n",
    "\n",
    "            for num in range(0, num_layers-1):\n",
    "                W = tf.Variable(dnn_weights[num])\n",
    "                b = tf.Variable(dnn_biases[num])\n",
    "                weights.append(W)\n",
    "                biases.append(b)\n",
    "                print('Loaded NN parameters successfully ...')\n",
    "\n",
    "        return weights, biases\n",
    "\n",
    "    def initialize_adaptive_weights(self):\n",
    "        # Capture gradient on each hidden layers\n",
    "        self.grad_collo = []\n",
    "        self.grad_boundary = []\n",
    "        for lyr in range(len(self.layers)-1):\n",
    "            self.grad_collo.append(tf.gradients(self.loss_collo, self.weights[lyr])[0])\n",
    "            self.grad_boundary.append(tf.gradients(self.loss_bound, self.weights[lyr])[0])\n",
    "\n",
    "        # Calculate the new weights\n",
    "        self.coef_bc_list = []\n",
    "        self.coef_bc_log = []\n",
    "        for lyr in range(len(self.layers)-1):\n",
    "            numerator_ = tf.reduce_max(tf.abs(self.grad_collo[lyr]))\n",
    "            denominator_ = tf.reduce_mean(tf.abs(self.grad_boundary[lyr]))\n",
    "            new_coef_bc = numerator_ / denominator_\n",
    "            self.coef_bc_list.append(new_coef_bc)\n",
    "        \n",
    "        self.coef_bc = tf.reduce_max(tf.stack(self.coef_bc_list))\n",
    "\n",
    "    def initialize_network(self):\n",
    "        # Initialize\n",
    "        weights = []\n",
    "        biases = []\n",
    "        num_layers = len(self.layers)\n",
    "\n",
    "        # Create network\n",
    "        for lyr in range(num_layers-1):\n",
    "            # initialize weights from Xavier initialization\n",
    "            np.random.seed(self.random_seed)\n",
    "            W = self.xavier_init(size=[self.layers[lyr], \n",
    "                                       self.layers[lyr+1]])\n",
    "\n",
    "            # initialize biases = 0\n",
    "            np.random.seed(self.random_seed)\n",
    "            b = tf.Variable(tf.zeros([1, self.layers[lyr+1]],\n",
    "                                     dtype=tf.float32),\n",
    "                            dtype=tf.float32)\n",
    "\n",
    "            # Append generated weights & biases to the list\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "\n",
    "        return weights, biases\n",
    "\n",
    "    def initialize_optimizers(self):\n",
    "        if self.problem_type == \"Inverse\":\n",
    "           variable_list = self.weights+self.biases+self.lambda_1\n",
    "        else:\n",
    "           variable_list = self.weights+self.biases\n",
    "        if self.adaptive_model:\n",
    "            self.global_step = tf.Variable(self.last_adam_iter, trainable=False)\n",
    "\n",
    "            lr_init = self.lr_init\n",
    "            self.lr_tf = tf.train.exponential_decay(lr_init,\n",
    "                                                    self.global_step,\n",
    "                                                    1000, 0.9, staircase=False)\n",
    "            \n",
    "            self.optimizer_adam = tf.train.AdamOptimizer(learning_rate=self.lr_tf)\n",
    "            \n",
    "            self.train_op_adam = self.optimizer_adam.minimize(self.loss_total,\n",
    "                                                              var_list=variable_list,\n",
    "                                                              global_step=self.global_step)\n",
    "        else:\n",
    "            self.lr_tf = tf.constant(self.lr_init, dtype=tf.float32)    \n",
    "            self.optimizer_adam = tf.train.AdamOptimizer(learning_rate=self.lr_tf)\n",
    "            self.train_op_adam = self.optimizer_adam.minimize(self.loss_total,\n",
    "                                                            var_list=variable_list)\n",
    "\n",
    "        self.train_op_newton = tf.contrib.opt.ScipyOptimizerInterface(\n",
    "                                    self.loss_total,\n",
    "                                    var_list = variable_list,\n",
    "                                    method = \"L-BFGS-B\",\n",
    "                                    options = {\"maxiter\": 100000,\n",
    "                                               \"maxfun\": 100000,\n",
    "                                               \"maxcor\": 50,\n",
    "                                               \"maxls\": 50,\n",
    "                                               \"ftol\": 1*np.finfo(float).eps})\n",
    "\n",
    "    def initialize_placeholders(self):\n",
    "        # Learning rate\n",
    "        self.lr_tf = tf.placeholder(tf.float32, shape=[])\n",
    "        self.coef_bc_tf = tf.placeholder(tf.float32, shape=self.coef_bc_val.shape)\n",
    "\n",
    "        # Test data\n",
    "        self.x_test_tf = tf.placeholder(tf.float32, shape=[None, self.x_test.shape[1]])\n",
    "        self.y_test_tf = tf.placeholder(tf.float32, shape=[None, self.y_test.shape[1]])\n",
    "        self.u_test_tf = tf.placeholder(tf.float32, shape=[None, self.u_test.shape[1]])\n",
    "        \n",
    "        # Predict data\n",
    "        self.x_tf = tf.placeholder(tf.float32, shape=[None, self.x_c.shape[1]])\n",
    "        self.y_tf = tf.placeholder(tf.float32, shape=[None, self.y_c.shape[1]])\n",
    "\n",
    "        # Collocation data\n",
    "        self.x_c_tf = tf.placeholder(tf.float32, shape=[None, self.x_c.shape[1]])\n",
    "        self.y_c_tf = tf.placeholder(tf.float32, shape=[None, self.y_c.shape[1]])\n",
    "\n",
    "        # Boundary data\n",
    "        # Inlet\n",
    "#         self.x_inlet_tf = tf.placeholder(tf.float32, shape=[None, self.x_inlet.shape[1]])\n",
    "#         self.y_inlet_tf = tf.placeholder(tf.float32, shape=[None, self.y_inlet.shape[1]])\n",
    "#         self.u_inlet_tf = tf.placeholder(tf.float32, shape=[None, self.u_inlet.shape[1]])\n",
    "        # self.v_inlet_tf = tf.placeholder(tf.float32, shape=[None, self.v_inlet.shape[1]])\n",
    "\n",
    "        # Outlet\n",
    "        self.x_meu_tf = tf.placeholder(tf.float32, shape=[None, self.x_meu.shape[1]])\n",
    "        self.y_meu_tf = tf.placeholder(tf.float32, shape=[None, self.y_meu.shape[1]])\n",
    "        self.u_meu_tf = tf.placeholder(tf.float32, shape=[None, self.u_meu.shape[1]])\n",
    "\n",
    "        # Wall\n",
    "        self.x_wall_tf = tf.placeholder(tf.float32, shape=[None, self.x_wall.shape[1]])\n",
    "        self.y_wall_tf = tf.placeholder(tf.float32, shape=[None, self.y_wall.shape[1]])\n",
    "        self.u_wall_tf = tf.placeholder(tf.float32, shape=[None, self.u_wall.shape[1]])\n",
    "        # self.v_wall_tf = tf.placeholder(tf.float32, shape=[None, self.v_wall.shape[1]])\n",
    "\n",
    "        # if self.bc_type == \"Neumann\":\n",
    "        #    self.rho_tf = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n",
    "        #    self.cp_tf  = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n",
    "        #    self.k_tf   = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n",
    "\n",
    "    def initialize_session(self):\n",
    "        tf_config = tf.ConfigProto(allow_soft_placement=True,\n",
    "                                   log_device_placement=True)\n",
    "        self.sess = tf.Session(config=tf_config)\n",
    "        # self.sess = tf.InteractiveSession(config=tf_config)\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    def initialize_variables(self):\n",
    "        # For saving loss\n",
    "        self.loss_total_log = []\n",
    "        self.loss_collo_log = []\n",
    "        self.loss_meu_log = []\n",
    "        self.loss_init_log  = []\n",
    "        self.loss_test_log = []\n",
    "        self.count = 0\n",
    "        self.last_adam_iter = 0\n",
    "        self.adam_started = False\n",
    "        self.newton_started = False\n",
    "        self.lambda_1 = [tf.Variable([0.5], dtype=tf.float32)]\n",
    "        # self.lambda_2 = tf.Variable([-6.0], dtype=tf.float32)\n",
    "        self.lambda_1_log = []\n",
    "\n",
    "    def net_dnn(self, x, y):\n",
    "        # Find results\n",
    "        X = tf.concat([x, y], 1)\n",
    "        results = self.net_forward(X)\n",
    "        \n",
    "        return results\n",
    "        # psi = results[:,0:1]\n",
    "        # p = results[:,1:2]\n",
    "\n",
    "        # # Find u & v from stream function (psi)\n",
    "        # u = tf.gradients(psi, y)[0] / self.sigma_y\n",
    "        # v = -tf.gradients(psi, x)[0] / self.sigma_x\n",
    "\n",
    "        \n",
    "\n",
    "    def net_forward(self, X):\n",
    "        if self.improved_model:\n",
    "            num_layers = len(self.weights)+1\n",
    "            H = X\n",
    "\n",
    "            encoder_1 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_1), self.encoder_biases_1))\n",
    "            encoder_2 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_2), self.encoder_biases_2))\n",
    "\n",
    "            for lyr in range(num_layers-2):\n",
    "                W = self.weights[lyr]\n",
    "                b = self.biases[lyr]\n",
    "                H = tf.math.multiply(tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_1) \\\n",
    "                    + tf.math.multiply(1 - tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_2)\n",
    "\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            Y = tf.add(tf.matmul(H, W), b)\n",
    "        else:\n",
    "            num_layers = len(self.weights)+1\n",
    "            H = X\n",
    "            # print(H)\n",
    "\n",
    "            for lyr in range(num_layers-2):\n",
    "                W = self.weights[lyr]\n",
    "                b = self.biases[lyr]\n",
    "                if self.act_fun == \"tanh\":\n",
    "                    H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "                elif self.act_fun == \"relu\":\n",
    "                    H = tf.nn.relu(tf.add(tf.matmul(H, W), b))\n",
    "                elif self.act_fun == \"swish\":\n",
    "                    H = tf.nn.swish(tf.add(tf.matmul(H, W), b))\n",
    "                elif self.act_fun == \"sigmoid\":\n",
    "                    H = tf.nn.sigmoid(tf.add(tf.matmul(H, W), b))\n",
    "                else:\n",
    "                    print('No activation function')\n",
    "\n",
    "            W = self.weights[-1]\n",
    "            b = self.biases[-1]\n",
    "            Y = tf.add(tf.matmul(H, W), b)\n",
    "\n",
    "        return Y\n",
    "\n",
    "    def net_physics(self, x, y):\n",
    "        # Find results from DNN\n",
    "        T = self.net_dnn(x, y)\n",
    "\n",
    "        # Temperature gradient\n",
    "        T_x = tf.gradients(T, x)[0] / self.sigma_x\n",
    "        T_xx = tf.gradients(T_x, x)[0] / self.sigma_x\n",
    "        T_t = tf.gradients(T, y)[0] / self.sigma_y\n",
    "        # Physic Var\n",
    "        rho = self.RHO\n",
    "        cp  = self.cp\n",
    "        k   = self.k\n",
    "        M   = self.M\n",
    "\n",
    "        # Physics error\n",
    "        if self.problem_type == \"Inverse\":\n",
    "           lambda_1 = self.lambda_1\n",
    "           x = self.x_c\n",
    "           y = self.y_c\n",
    "           # x = (self.x_c - self.mu_x) / self.sigma_x\n",
    "           # y = (self.y_c - self.mu_y) / self.sigma_y\n",
    "           f = lambda_1*T_t - T_xx - np.exp(x + 2*y)\n",
    "        else:\n",
    "           if self.bc_type == 'Dirichlet':\n",
    "              x = self.x_c\n",
    "              y = self.y_c\n",
    "              # x = (self.x_c - self.mu_x) / self.sigma_x\n",
    "              # y = (self.y_c - self.mu_y) / self.sigma_y\n",
    "              f = T_t - T_xx - np.exp(x + 2*y)\n",
    "           elif self.bc_type == 'Neumann':\n",
    "              # f = rho*cp*T_t - k*T_xx\n",
    "              k_ = (T - k[0,0])/(k[1,0] - k[0,0])*(k[1,1] - k[0,1]) + k[0,1]\n",
    "              rho_ = (T - rho[0,0])/(rho[1,0] - rho[0,0])*(rho[1,1] - rho[0,1]) + rho[0,1]\n",
    "              cp_ = (T - cp[0,0])/(cp[1,0] - cp[0,0])*(cp[1,1] - cp[0,1]) + cp[0,1]\n",
    "              f = M*T_t - M*T_xx*k_/(rho_*cp_)\n",
    "\n",
    "        return f\n",
    "    def normalize_data(self, data, axis):\n",
    "        if axis == \"x\":\n",
    "            normalized_data = (data - self.mu_x) / self.sigma_x\n",
    "            # normalized_data = data\n",
    "            # normalized_data = data / self.ub[0]\n",
    "        elif axis == \"y\":\n",
    "            normalized_data = (data - self.mu_y) / self.sigma_y\n",
    "            # normalized_data = data\n",
    "            # normalized_data = data / self.ub[1]\n",
    "\n",
    "        return normalized_data\n",
    "\n",
    "    def predict(self, x_star, y_star):\n",
    "        # Prepare the input\n",
    "        x_star = (x_star - self.mu_x) / self.sigma_x\n",
    "        y_star = (y_star - self.mu_y) / self.sigma_y\n",
    "\n",
    "        # x_star = x_star / self.ub[0]\n",
    "        # y_star = y_star / self.ub[1]\n",
    "\n",
    "        # Create dictionary\n",
    "        tf_dict = {self.x_tf:x_star, self.y_tf:y_star}\n",
    "\n",
    "        # Predict\n",
    "        u_star = self.sess.run(self.u_pred, tf_dict)\n",
    "        # v_star = self.sess.run(self.v_pred, tf_dict)\n",
    "        # p_star = self.sess.run(self.p_pred, tf_dict)\n",
    "\n",
    "        return u_star\n",
    "\n",
    "    def save_loss(self, file_dir):\n",
    "        loss_test = np.array(self.loss_test_log)\n",
    "        loss_data = np.column_stack((self.loss_total_log, \n",
    "                                     self.loss_collo_log,\n",
    "                                     self.loss_bound_log,\n",
    "                                     self.loss_init_log,\n",
    "                                     loss_test))\n",
    "        loss_df = pd.DataFrame(loss_data, columns=[\"total\", \"collo\", \"boundary\", \"initial\", \"error_u\"])\n",
    "        joblib.dump(loss_df, file_dir)\n",
    "\n",
    "    def save_model(self, file_dir):\n",
    "        weights = self.sess.run(self.weights)\n",
    "        biases = self.sess.run(self.biases)\n",
    "\n",
    "        with open(file_dir, 'wb') as f:\n",
    "            pickle.dump([weights, biases], f)\n",
    "            print(\"Save NN parameters successfully...\")\n",
    "\n",
    "    def test_loss(self):\n",
    "        # Prediction\n",
    "        # u_pred, v_pred, p_pred = self.predict(self.x_test, self.y_test)\n",
    "        u_pred = self.predict(self.x_test, self.y_test)\n",
    "        error_u = np.linalg.norm(u_pred - self.u_test) / np.linalg.norm(self.u_test)\n",
    "\n",
    "        return error_u\n",
    "\n",
    "    def unpack(self, data, params):\n",
    "        # Initialize\n",
    "        self.data = data\n",
    "        self.params = params\n",
    "\n",
    "        # Unpack Parameters\n",
    "        # Data-Boundary\n",
    "        self.lb = params[\"data\"][\"lb\"]\n",
    "        self.ub = params[\"data\"][\"ub\"]\n",
    "        \n",
    "        self.random_seed = data[\"train\"][\"random_seed\"]\n",
    "        \n",
    "        # Data-Collocation\n",
    "        self.x_c = data[\"train\"][\"collo\"][:, 0:1]\n",
    "        self.y_c = data[\"train\"][\"collo\"][:, 1:2]\n",
    "        self.mu_x = data[\"train\"][\"mu_x\"]\n",
    "        self.mu_y = data[\"train\"][\"mu_y\"]\n",
    "        self.sigma_x = data[\"train\"][\"sigma_x\"]\n",
    "        self.sigma_y = data[\"train\"][\"sigma_y\"]\n",
    "        \n",
    "        # Measurements\n",
    "        self.loc_num = params[\"data\"][\"loc\"]\n",
    "        self.meu_num = params[\"data\"][\"meu\"]\n",
    "#         meu = np.zeros(shape=(1, 3))\n",
    "#         for i in range(0, len(self.loc_num) ):\n",
    "#             meu = np.concatenate((meu, self.data[\"train\"][f\"loc={self.loc_num[i]}\"]), axis=0)\n",
    "            \n",
    "#         meu = meu[1:,:]\n",
    "        meu = data[\"train\"][\"mea\"]\n",
    "        self.x_meu = meu[:, 0:1]\n",
    "        self.y_meu = meu[:, 1:2]\n",
    "        self.u_meu = meu[:, 2:3]\n",
    "        \n",
    "        # Data-Inlet\n",
    "#         self.x_inlet = data[\"train\"][\"inlet\"][:, 0:1]\n",
    "#         self.y_inlet = data[\"train\"][\"inlet\"][:, 1:2]\n",
    "#         self.u_inlet = data[\"train\"][\"inlet\"][:, 2:3]\n",
    "        # self.v_inlet = data[\"train\"][\"inlet\"][:, 3:4]\n",
    "        # self.p_inlet = data[\"train\"][\"inlet\"][:, 4:5]\n",
    "\n",
    "        # Data-Outlet\n",
    "#         self.x_outlet = data[\"train\"][\"outlet\"][:, 0:1]\n",
    "#         self.y_outlet = data[\"train\"][\"outlet\"][:, 1:2]\n",
    "#         self.u_outlet = data[\"train\"][\"outlet\"][:, 2:3]\n",
    "        # self.v_outlet = data[\"train\"][\"outlet\"][:, 3:4]\n",
    "        # self.p_outlet = data[\"train\"][\"outlet\"][:, 4:5]\n",
    "        \n",
    "        # Data-Wall\n",
    "        self.x_wall = data[\"train\"][\"wall\"][:, 0:1]\n",
    "        self.y_wall = data[\"train\"][\"wall\"][:, 1:2]\n",
    "        self.u_wall = data[\"train\"][\"wall\"][:, 2:3]\n",
    "        # self.v_wall = data[\"train\"][\"wall\"][:, 3:4]\n",
    "        # self.p_wall = data[\"train\"][\"wall\"][:, 4:5]\n",
    "\n",
    "        # Data-Test\n",
    "        self.x_test = data[\"test\"][:, 0:1]\n",
    "        self.y_test = data[\"test\"][:, 1:2]\n",
    "        self.u_test = data[\"test\"][:, 2:3]\n",
    "        # self.v_test = data[\"test\"][:, 3:4]\n",
    "        # self.p_test = data[\"test\"][:, 4:5]\n",
    "\n",
    "        # Data-Others\n",
    "        self.n_collo = params[\"data\"][\"n_collo\"]\n",
    "        self.M = params[\"data\"][\"M\"]\n",
    "        self.x = np.linspace(0,0.1,101).flatten()[:,None]\n",
    "        \n",
    "        # # Physic\n",
    "        self.RHO = params[\"physic\"][\"rho\"]\n",
    "        self.cp  = params[\"physic\"][\"cp\"]\n",
    "        self.k   = params[\"physic\"][\"k\"]\n",
    "        # self.MU = params[\"physic\"][\"mu\"]\n",
    "\n",
    "        # Network\n",
    "        self.epochs = params[\"network\"][\"epochs\"]\n",
    "        self.layers = params[\"network\"][\"layers\"]\n",
    "        self.coef_bc_val = np.array(params[\"network\"][\"coef_bc\"])\n",
    "        self.lr_init = params[\"network\"][\"lr_init\"]\n",
    "        self.beta = params[\"network\"][\"beta\"]\n",
    "        self.batch_size = params[\"network\"][\"batches\"]\n",
    "        self.verboses_adam = params[\"network\"][\"verboses_adam\"]\n",
    "        self.verboses_newton = params[\"network\"][\"verboses_newton\"]\n",
    "        self.verboses_adaptive = params[\"network\"][\"verboses_adaptive\"]\n",
    "        self.saver = params[\"network\"][\"saver\"]\n",
    "        self.act_fun = params[\"network\"][\"act_fun\"]\n",
    "\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
    "\n",
    "        # tf.set_random_seed(20)\n",
    "        np.random.seed(self.random_seed)\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], \n",
    "                                               stddev=xavier_stddev, \n",
    "                                               dtype=tf.float32,\n",
    "                                               seed=self.random_seed), \n",
    "                           dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-14xgy3Q8gk7"
   },
   "source": [
    "### **Cases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1646374737175,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "SBoYWjnT8iQt"
   },
   "outputs": [],
   "source": [
    "class CasesChannel:\n",
    "\n",
    "    def __init__(self, add_noise=False, save_fig=False):\n",
    "        self.add_noise = add_noise\n",
    "        self.save_fig = save_fig\n",
    "        self.data = {}\n",
    "\n",
    "    # def analytics_solution(self, y):\n",
    "    def analytics_solution(self, x, t):\n",
    "        T_ = np.exp(x+2*t)\n",
    "\n",
    "        return T_\n",
    "\n",
    "    def generate_bc(self, loc, n):\n",
    "        x = loc*self.ub[0]*np.ones((n,1))\n",
    "        np.random.seed(self.random_seed)  #INI BEDA\n",
    "        t = lhs(1,n)*self.ub[1]\n",
    "        T = self.analytics_solution(x, t)\n",
    "        # print(T)\n",
    "\n",
    "        # concat data\n",
    "        bc_datas = np.concatenate((x, t, T), axis=1)\n",
    "        \n",
    "        return bc_datas\n",
    "\n",
    "    def generate_ic(self):\n",
    "        n = self.n_wall\n",
    "        np.random.seed(self.random_seed)      #INI BEDA\n",
    "        x = lhs(1,n)*self.ub[0]\n",
    "        t = 0.0*lhs(1,n)\n",
    "        T = self.analytics_solution(x, t)\n",
    "\n",
    "        # concat data\n",
    "        ic_datas = np.concatenate((x, t, T), axis=1)\n",
    "#         print(ic_datas[:,0].shape)\n",
    "        return ic_datas #, random_noise\n",
    "        \n",
    "    \n",
    "    def generate_collo(self):\n",
    "        # unpack\n",
    "        n = self.n_collo\n",
    "        lb = self.lb\n",
    "        ub = self.ub\n",
    "\n",
    "        # create points\n",
    "        np.random.seed(self.random_seed)\n",
    "        collo_pts = lb + (ub-lb)*lhs(2, n, criterion=None)\n",
    "\n",
    "        self.mu_X, self.sigma_X = collo_pts.mean(0), collo_pts.std(0)\n",
    "        self.mu_x, self.sigma_x = self.mu_X[0], self.sigma_X[0]\n",
    "        self.mu_y, self.sigma_y = self.mu_X[1], self.sigma_X[1]\n",
    "                \n",
    "        return collo_pts \n",
    "\n",
    "    def generate_train_data(self, param, bc_type):\n",
    "        self.bc_type = bc_type\n",
    "        \n",
    "        # Unpack parameters\n",
    "        self.unpack(param)\n",
    "        \n",
    "\n",
    "        # Generate points\n",
    "        # Generate boundary conditions data \n",
    "        init_cond = self.generate_ic()\n",
    "        \n",
    "        loc = self.loc_num   # location measurements --> e.g 0%, 50%, 100%\n",
    "        meu = self.meu_num   # Measurements --> 50, 100, 150..\n",
    "        \n",
    "        bc = []\n",
    "#         noise_bc = []\n",
    "        \n",
    "        self.data[\"train\"] = {}\n",
    "        for i in range (0, len(loc)):\n",
    "            mea = self.generate_bc(loc[i],int(meu[i]))\n",
    "            bc.append(mea)\n",
    "#             noise_bc.append(mea_noise)\n",
    "            self.data[\"train\"][f\"loc={loc[i]}\"] = bc[i]\n",
    "#             self.data[\"train\"][f\"noise_loc={loc[i]}\"] = noise_bc[i]\n",
    "            \n",
    "        mea = np.zeros(shape=(1, 3))\n",
    "        for i in range(0, len(self.loc_num) ):\n",
    "            mea = np.concatenate((mea, self.data[\"train\"][f\"loc={self.loc_num[i]}\"]), axis=0)\n",
    "            \n",
    "        mea = mea[1:,:]\n",
    "#         self.x_meu = meu[:, 0:1]\n",
    "#         self.y_meu = meu[:, 1:2]\n",
    "        T = mea[:, 2:3]\n",
    "        T_init = init_cond[:,2:3]\n",
    "        self.T = T\n",
    "        self.T_init= T_init\n",
    "            \n",
    "        # noise addition\n",
    "        if self.add_noise:\n",
    "            noise = self.noise_level\n",
    "            maxT = max([np.max(T), np.max(T_init)])\n",
    "            sigma = noise * maxT\n",
    "            print(f\"T max: {maxT}\")\n",
    "            np.random.seed(self.random_seed)\n",
    "            random_noise_T = sigma*np.random.randn(T.shape[0],1)\n",
    "            \n",
    "            T = T + random_noise_T\n",
    "            \n",
    "            random_noise_init = sigma*np.random.randn(T_init.shape[0], 1)\n",
    "            \n",
    "            T_init = T_init + random_noise_init\n",
    "            \n",
    "        self.random_noise_T = random_noise_T\n",
    "        self.random_noise_init = random_noise_init\n",
    "            \n",
    "        self.data[\"train\"][\"mea\"] = np.concatenate((mea[:, 0:2], T), axis=1)         \n",
    "        self.data[\"train\"][\"wall\"] = np.concatenate((init_cond[:, 0:2], T_init), axis=1)\n",
    "            \n",
    "\n",
    "        # Generate collocations points data\n",
    "        collo_ = self.generate_collo()\n",
    "\n",
    "        # Pack data\n",
    "#         self.data[\"train\"] = {}\n",
    "        self.data[\"train\"][\"collo\"] = collo_\n",
    "#         self.data[\"train\"][\"inlet\"] = left_bc\n",
    "#         self.data[\"train\"][\"meu\"] = right_bc\n",
    "#          = init_cond\n",
    "        self.data[\"train\"][\"mu_x\"] = self.mu_x\n",
    "        self.data[\"train\"][\"sigma_x\"] = self.sigma_x\n",
    "        self.data[\"train\"][\"mu_y\"] = self.mu_y\n",
    "        self.data[\"train\"][\"sigma_y\"] = self.sigma_y\n",
    "        self.data[\"train\"][\"random_seed\"] = self.random_seed\n",
    "\n",
    "    def generate_test_data(self, param):\n",
    "        # Unpack parameters\n",
    "        self.unpack(param)\n",
    "\n",
    "        # Generate points\n",
    "        length_ = self.ub - self.lb\n",
    "        min_length_ = np.argmin(length_)\n",
    "        frac_ = max(length_) / min(length_)\n",
    "        n_test_1 = int(frac_ * self.n_test)\n",
    "\n",
    "        # Create grid\n",
    "        if min_length_ == 0:\n",
    "            self.n_x = self.n_test\n",
    "            self.n_y = n_test_1\n",
    "        else:\n",
    "            self.n_x = n_test_1\n",
    "            self.n_y = self.n_test\n",
    "        if self.bc_type == \"Dirichlet\":\n",
    "            x_ = np.linspace(self.lb[0], self.ub[0], num=(self.n_x)) # tadinya n_test\n",
    "            # x_add = np.column_stack((x_1, x_2))\n",
    "            y_ = np.linspace(self.lb[1], self.ub[1], num=int(self.n_y/self.n_test))\n",
    "            X, Y = np.meshgrid(x_, y_)\n",
    "            X_flat = X.flatten()[:,None]\n",
    "            Y_flat = Y.flatten()[:,None]\n",
    "            T_ = self.analytics_solution(X_flat, Y_flat)\n",
    "        elif self.bc_type == \"Neumann\":\n",
    "            u_fea = pd.read_csv('import_test_t2160.csv')\n",
    "            u_fea = u_fea.to_numpy()\n",
    "            # x_ = np.linspace(self.lb[0], self.ub[0], num=self.n_x)\n",
    "            x_ = u_fea[:,0:1]\n",
    "            y_ = self.t_check\n",
    "            X, Y = np.meshgrid(x_, y_)\n",
    "            X_flat = X.flatten()[:,None]\n",
    "            Y_flat = Y.flatten()[:,None]\n",
    "            T_ = u_fea[:,1:2]\n",
    "            # T_ = u_fea[:,1:2] + 273.0\n",
    "        # T_1 = self.analytics_solution(x_1,y_)\n",
    "        # T_2 = self.analytics_solution(x_2,y_)\n",
    "        # T_add = np.column_stack((T_1, T_2))\n",
    "        # v_ = np.zeros(self.n_test)\n",
    "\n",
    "        # self.data[\"test\"] = np.column_stack((x_, y_, u_, v_))\n",
    "        self.data[\"test\"] = np.column_stack((X_flat, Y_flat, T_))\n",
    "        # self.data[\"test_add\"] = np.column_stack((x_1, x_2, y_, T_1, T_2))\n",
    "\n",
    "    def plot(self):\n",
    "        # unpack data\n",
    "        collo_ = self.data[\"train\"][\"collo\"]\n",
    "        meu = []\n",
    "        \n",
    "        for i in range(0, len(self.loc_num)):\n",
    "#             print(self.data[\"train\"])\n",
    "#                        self.data[\"train\"][f\"loc={loc[i]}\"]\n",
    "            meu.append(self.data[\"train\"][f\"loc={self.loc_num[i]}\"])\n",
    "#         inlet_ = self.data[\"train\"][\"inlet\"]\n",
    "#         outlet_ = self.data[\"train\"][\"outlet\"]\n",
    "        wall_ = self.data[\"train\"][\"wall\"]\n",
    "\n",
    "        # PLOT: points distribution\n",
    "        # Properties\n",
    "        plt.rc('text', usetex=True)\n",
    "        plt.rc('font', family='serif')\n",
    "\n",
    "        # Plot\n",
    "        if self.bc_type == \"Dirichlet\":\n",
    "            fig_w = 5\n",
    "            fig_h = 5\n",
    "        elif self.bc_type == \"Neumann\":\n",
    "            fig_w = 3\n",
    "            fig_h = 10\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(fig_w, fig_h), constrained_layout=True, dpi=300)\n",
    "\n",
    "        ax.plot([self.lb[0], self.ub[0]], [self.lb[1], self.lb[1]], 'k')\n",
    "        ax.plot([self.lb[0], self.ub[0]], [self.ub[1], self.ub[1]], 'k')\n",
    "        ax.plot([self.lb[0], self.lb[0]], [self.ub[1], self.lb[1]], 'k')\n",
    "        ax.plot([self.ub[0], self.ub[0]], [self.ub[1], self.lb[1]], 'k')\n",
    "\n",
    "        ax.scatter(collo_[:,0:1], collo_[:,1:2], marker='.', alpha=0.7, c='grey', label='Collo')\n",
    "        for i in range(0, len(self.loc_num)):\n",
    "            ax.scatter(meu[i][:,0:1], meu[i][:,1:2], marker='.', alpha=0.7,  label=f'Measurements, $x/L$={np.round(self.loc_num[i],2)}')\n",
    "#         ax.scatter(inlet_[:,0:1], inlet_[:,1:2], marker='.', alpha=0.7, c='r', label='Left BC')\n",
    "#         ax.scatter(outlet_[:,0:1], outlet_[:,1:2], marker='.', alpha=0.7, c='g', label='Right BC')\n",
    "        ax.scatter(wall_[:,0:1], wall_[:,1:2], marker='.', alpha=0.7, c='b', label='IC')\n",
    "\n",
    "        ax.set_title(\"Points distribution\", fontsize=15)\n",
    "        ax.set_xlabel(\"$x$ (m)\", fontsize=20)\n",
    "        ax.set_ylabel(\"$t$ (s)\", fontsize=20)\n",
    "#         ax.set_xlabel(\"x (m)\", fontsize=12)\n",
    "#         ax.set_ylabel(\"t (s)\", fontsize=12)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "        ax.legend(fontsize=10, loc=4)\n",
    "        ax.grid(linestyle=\"--\")\n",
    "        ax.set_xlim(self.lb[0], self.ub[0])\n",
    "        ax.set_ylim(self.lb[1], self.ub[1])\n",
    "\n",
    "        if self.save_fig:\n",
    "            fig.savefig(\"fig_point_distribution.eps\", format=\"eps\")\n",
    "        plt.show()\n",
    "\n",
    "        # # PLOT: test data\n",
    "        # fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 3), constrained_layout=True, dpi=300)\n",
    "\n",
    "        # # Inlet velocity\n",
    "        # x_ = 0.5*(self.ub[0]-self.lb[0])*np.ones(self.n_test)\n",
    "        # y_ = np.linspace(self.lb[1], self.ub[1], self.n_inlet)\n",
    "        # u_analytics = self.analytics_solution(x_,y_)\n",
    "\n",
    "        # ax.plot(u_analytics, y_, 'b', label='fully-developed flow')\n",
    "        # ax.scatter(inlet_[:, 2:3], inlet_[:, 1:2], c='r', marker='.', label='inlet')\n",
    "\n",
    "        # ax.set_title(\"Velocity Comparison\", fontsize=20)\n",
    "        # #ax.set_xlabel('$u$ (m/s)', fontsize=20)\n",
    "        # #ax.set_ylabel('$y$ (m)', fontsize=20)\n",
    "        # ax.set_xlabel('u (m/s)', fontsize=20)\n",
    "        # ax.set_ylabel('y (m)', fontsize=20)\n",
    "        # ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "        # ax.legend(fontsize=12)\n",
    "        # ax.grid(linestyle=\"--\")\n",
    "        # ax.set_ylim([self.lb[1], self.ub[1]])\n",
    "\n",
    "        if self.save_fig:\n",
    "            fig.savefig(\"fig_references.eps\", format=\"eps\")\n",
    "        plt.show()\n",
    "\n",
    "    def unpack(self, param):\n",
    "        # # Constant\n",
    "        # self.mu = param[\"physic\"][\"mu\"]\n",
    "        # self.u_max = param[\"physic\"][\"u_max\"]\n",
    "        # self.u_wall = param[\"physic\"][\"u_wall\"]\n",
    "\n",
    "        # self.T_left = param[\"physic\"][\"T_left\"]\n",
    "        # self.T_right = param[\"physic\"][\"T_right\"]\n",
    "        # self.T_lower = param[\"physic\"][\"T_lower\"]\n",
    "        # self.T_upper = param[\"physic\"][\"T_upper\"]\n",
    "\n",
    "        # Bound\n",
    "        self.lb = param[\"data\"][\"lb\"]\n",
    "        self.ub = param[\"data\"][\"ub\"]\n",
    "\n",
    "        # Discretizations\n",
    "        self.n_collo = param[\"data\"][\"n_collo\"]\n",
    "#         self.n_inlet = param[\"data\"][\"n_inlet\"]\n",
    "#         self.n_outlet = param[\"data\"][\"n_outlet\"]\n",
    "        self.n_wall = param[\"data\"][\"n_wall\"]\n",
    "        self.n_test = param[\"data\"][\"n_test\"]\n",
    "        self.init_wall = param[\"data\"][\"init_wall\"]\n",
    "        self.left_q = param[\"data\"][\"left_q\"]\n",
    "        self.right_q = param[\"data\"][\"right_q\"]\n",
    "        self.t_check = param[\"data\"][\"t_check\"]\n",
    "        self.random_seed = param[\"data\"][\"seed\"]\n",
    "        self.noise_level = param[\"data\"][\"noise\"]\n",
    "        self.loc_num = param[\"data\"][\"loc\"]\n",
    "        self.meu_num = param[\"data\"][\"meu\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpZLqlZx8isK"
   },
   "source": [
    "### **Post Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2115,
     "status": "ok",
     "timestamp": 1646374739288,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "nZwRPWz48kf4"
   },
   "outputs": [],
   "source": [
    "class PostChannel:\n",
    "\n",
    "    def __init__(self, model, params, save_fig=False):\n",
    "        self.save_fig = save_fig\n",
    "        self.unpack(model, params)\n",
    "\n",
    "    def calculate_pinn(self):\n",
    "        \n",
    "        # self.u_pinn, self.v_pinn, self.p_pinn = self.model.predict(self.X_flat, self.Y_flat)\n",
    "        # self.u_pinn = self.model.predict(self.X_flat, self.Y_flat/self.ub[1])\n",
    "        if self.model.bc_type == \"Neumann\":\n",
    "            n_y = self.n_y_dummy\n",
    "        elif self.model.bc_type == \"Dirichlet\":\n",
    "            n_y = self.n_y\n",
    "        self.u_pinn = self.model.predict(self.X_flat, self.Y_flat)\n",
    "        self.x_pinn_ = self.X_flat.reshape(n_y, self.n_x)\n",
    "        self.y_pinn_ = self.Y_flat.reshape(n_y, self.n_x)\n",
    "        self.u_pinn_ = self.u_pinn.reshape(n_y, self.n_x)\n",
    "        # self.v_pinn_ = self.v_pinn.reshape(self.n_y, self.n_x)\n",
    "        # self.p_pinn_ = self.p_pinn.reshape(self.n_y, self.n_x)\n",
    "\n",
    "    def calculate_error(self, n_data):\n",
    "        # X & Y\n",
    "        x_test = self.model.x_test # x = 0.25\n",
    "        y_test = self.model.y_test\n",
    "\n",
    "        # Predict\n",
    "        # u_pinn, _, _ = self.model.predict(x_test, y_test)\n",
    "        # u_pinn = self.model.predict(x_test, y_test)\n",
    "        \n",
    "        # # Find error\n",
    "        # del_u = u_pinn - u_test\n",
    "        # rmse_u = np.linalg.norm(del_u) / np.linalg.norm(u_test)\n",
    "\n",
    "\n",
    "        # Prediction\n",
    "        u_pinn = self.u_pinn\n",
    "\n",
    "        if self.model.bc_type == 'Dirichlet':\n",
    "            # u_analytic = case.analytics_solution(self.X_flat, self.Y_flat)\n",
    "            u_analytic = np.exp(self.X_flat+2*self.Y_flat)\n",
    "            u_test = u_analytic\n",
    "            self.u_test = u_test\n",
    "            # u_test = self.model.u_test\n",
    "        elif self.model.bc_type == 'Neumann':\n",
    "            u_fea = pd.read_csv('import_test_t2160.csv')\n",
    "            u_test = u_fea\n",
    "\n",
    "        delta_u = np.abs(u_pinn - u_test)\n",
    "        #  Absolute error with n_x and n_y points\n",
    "        self.abs_err_u = np.sum(delta_u)/(self.n_x * self.n_y)\n",
    "\n",
    "        rel_err_u_ij = delta_u/u_test\n",
    "        # Relative error with n_x and n_y points\n",
    "        self.rel_err_u = np.sum(rel_err_u_ij)/(self.n_x * self.n_y)\n",
    "        lambda_ = self.model.lambda_1_log[-1][0][0]\n",
    "#         lambda_2 = self.model.lambda_2_log[-1][0][0]\n",
    "        \n",
    "        self.rel_err_lambda = np.abs(lambda_ - 1.0) / 1.0\n",
    "#         self.rel_err_lambda_2 = np.abs(lambda_2 - 1.0) / 1.0\n",
    "        \n",
    "\n",
    "        # rmse_total = rmse_u\n",
    "        # print(f\"- RMSE x 10^(-2): {rmse_total/(1.e-2):5f}\")\n",
    "        \n",
    "        print(f\"- Relative Error Lambda (%): {self.rel_err_lambda*100:5f}\")\n",
    "\n",
    "        # rmse_total = rmse_u\n",
    "        # print(f\"- RMSE x 10^(-2): {rmse_total/(1.e-2):5f}\")\n",
    "        print(f\"- Absolute Error: {self.abs_err_u:5f}\")\n",
    "        print(f\"- Relative Error (%): {self.rel_err_u*100:5f}\")\n",
    "        \n",
    "\n",
    "        return x_test, y_test, u_pinn, u_test\n",
    "\n",
    "    def create_test_data(self):\n",
    "        # Find fraction\n",
    "        length_ = self.ub - self.lb\n",
    "        min_length_ = np.argmin(length_)\n",
    "        frac_ = max(length_) / min(length_)\n",
    "        n_test_1 = int(frac_ * self.n_test)\n",
    "\n",
    "        # Create grid\n",
    "        if min_length_ == 0:\n",
    "            self.n_x = self.n_test\n",
    "            self.n_y = n_test_1\n",
    "        else:\n",
    "            self.n_x = n_test_1\n",
    "            self.n_y = self.n_test\n",
    "\n",
    "        if self.model.bc_type == 'Dirichlet':\n",
    "            self.x = np.linspace(self.lb[0], self.ub[0], num=self.n_x)\n",
    "            self.y = np.linspace(self.lb[1], self.ub[1], num=self.n_y)\n",
    "            self.X, self.Y = np.meshgrid(self.x, self.y)\n",
    "            self.X_flat = self.X.flatten()[:, None]\n",
    "            self.Y_flat = self.Y.flatten()[:, None]\n",
    "        elif self.model.bc_type == 'Neumann':\n",
    "            self.x = np.linspace(self.lb[0], self.ub[0], num=self.n_x)\n",
    "            # self.y = 2160*np.ones(self.n_x)\n",
    "            self.n_y_dummy = int(self.n_y / self.n_test)\n",
    "            self.y = np.linspace(self.lb[1], self.ub[1], num=self.n_y_dummy)\n",
    "            self.X, self.Y = np.meshgrid(self.x, self.y)\n",
    "            self.X_flat = self.X.flatten()[:,None]\n",
    "            self.Y_flat = self.Y.flatten()[:,None]\n",
    "\n",
    "    def display_loss(self):\n",
    "        # SET\n",
    "        #plt.rc('text', usetex=True)\n",
    "        #plt.rc('font', family='serif')\n",
    "\n",
    "        # Extract data\n",
    "        loss_total = np.sqrt(self.model.loss_total_log)\n",
    "        loss_collo = np.sqrt(self.model.loss_collo_log)\n",
    "        loss_meu = np.sqrt(self.model.loss_meu_log)\n",
    "        loss_init = np.sqrt(self.model.loss_init_log)\n",
    "        loss_test = self.model.loss_test_log\n",
    "\n",
    "        # Status\n",
    "        if self.model.adam_started==True and self.model.newton_started==True:\n",
    "            run_status = \"full\"\n",
    "        elif self.model.adam_started:\n",
    "            run_status = \"adam\"\n",
    "        elif self.model.newton_started:\n",
    "            run_status = \"newton\"\n",
    "        else:\n",
    "            run_status = \"none\"\n",
    "\n",
    "        # Prepare\n",
    "        iter_total = []\n",
    "\n",
    "        if run_status == \"full\":\n",
    "            n_total = len(loss_total)\n",
    "            n_adam = int(self.model.last_adam_iter / self.model.verboses_adam)\n",
    "            n_newton = n_total - n_adam\n",
    "\n",
    "            for i in range(n_total):\n",
    "                if i < n_adam:\n",
    "                    iter_total.append(i * self.verboses_adam)\n",
    "                else:\n",
    "                    iter_total.append(self.model.last_adam_iter + i)\n",
    "        elif run_status == \"adam\":\n",
    "            n_total = int(self.model.last_adam_iter // self.model.verboses_adam)\n",
    "\n",
    "            if self.model.last_adam_iter % self.model.verboses_adam != 0:\n",
    "                n_total += self.model.last_adam_iter % self.model.verboses_adam\n",
    "\n",
    "            for i in range(n_total):\n",
    "                iter_total.append(i * self.verboses_adam)\n",
    "        elif run_status == \"newton\":\n",
    "            n_total = len(loss_total)\n",
    "\n",
    "            for i in range(n_total):\n",
    "                iter_total.append(i)\n",
    "        else:\n",
    "            n_total = 0\n",
    "        \n",
    "        # PLOT History\n",
    "        if run_status != \"none\":\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6), constrained_layout=True, dpi=300)\n",
    "            ax.plot(iter_total, loss_total, \"r\", linestyle=\"solid\", label=\"Physical Loss\")\n",
    "            ax.plot(iter_total, loss_collo, \"g\", linestyle=\"dashdot\", label=\"Collocation Loss\")\n",
    "            ax.plot(iter_total, loss_meu, \"b\", linestyle=\"dotted\", label=\"Measurement Loss\")\n",
    "            ax.plot(iter_total, loss_init, 'purple',linestyle='solid', label='Initial Loss')\n",
    "            ax.plot(iter_total, loss_test, \"black\", linestyle=\"dashed\", label=\"Test Error\")\n",
    "\n",
    "            if run_status == \"full\":\n",
    "                min_ = min(min(loss_total, loss_collo, loss_bound, loss_test))\n",
    "                min_ = min_ - 10*min_\n",
    "                max_ = max(max(loss_total, loss_collo, loss_bound, loss_test))\n",
    "                max_ = max_ + 10*max_\n",
    "\n",
    "                x_ = self.model.last_adam_iter\n",
    "                ax.plot([x_, x_], [min_, max_], \"cyan\", linestyle=\"dashed\")\n",
    "\n",
    "            #ax.set_title(\"Loss History\", fontsize=20)\n",
    "            ax.set_xlim(0, iter_total[-1])\n",
    "            ax.set_yscale(\"log\")\n",
    "            ax.set_xlabel(\"Iterations\", fontsize=12)\n",
    "            ax.set_ylabel(\"RMSE\", fontsize=12)\n",
    "            ax.grid(linestyle=\"--\")\n",
    "            ax.legend(fontsize=10)\n",
    "            ax.set_title('Loss History (Log Scale)', fontsize=17)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "            # save figure\n",
    "            if self.save_fig:\n",
    "                fig.savefig(\"fig_loss_history.eps\", format=\"eps\")\n",
    "            plt.show()\n",
    "\n",
    "        # PLOT Weights history\n",
    "        if self.model.adaptive_model:\n",
    "            adaptive_weights = self.model.coef_bc_log\n",
    "            iter_weights = [i*self.verboses_adaptive for i in range(len(adaptive_weights))]\n",
    "\n",
    "            # Create figure\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4, 4), constrained_layout=False, dpi=300)\n",
    "            \n",
    "            ax.plot(iter_weights, adaptive_weights, \"r\", linestyle=\"solid\", label=\"Boundary Weights\")\n",
    "            ax.set_title(\"Boundary Weights\", fontsize=10)\n",
    "            ax.set_xlim(min(iter_weights), max(iter_weights))\n",
    "            ax.set_xlabel(\"Iterations\", fontsize=10)\n",
    "            ax.set_ylabel(\"Boundary Weights\", fontsize=10)\n",
    "            ax.grid(linestyle=\"--\")\n",
    "            ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "            ax.legend(fontsize=6)\n",
    "            \n",
    "            # save figure\n",
    "            if self.save_fig:\n",
    "                fig.savefig(\"fig_weights_history.eps\", format=\"eps\")\n",
    "            plt.show()\n",
    "        \n",
    "        print(f\"- Last Iterations: {iter_total[-1]}\")\n",
    "        \n",
    "    def display_contour(self):\n",
    "        # FIND: PINN results\n",
    "        self.create_test_data()\n",
    "        self.calculate_pinn()\n",
    "\n",
    "        # PLOT CONTOUR\n",
    "        fig_h = 3 \n",
    "        fig_w = 10\n",
    "\n",
    "        # CALCULATE: error\n",
    "        if self.model.bc_type == \"Dirichlet\":\n",
    "            x_, y_, u_pinn, u_test = self.calculate_error(n_data = self.n_test)\n",
    "            # PLOT: comparison\n",
    "            self.plot_comparison(x=self.x, y=self.y, \n",
    "                                phi_pinn=u_pinn.reshape(self.n_y, self.n_x), \n",
    "                                phi_analytics=u_test.reshape(self.n_y, self.n_x), \n",
    "                                fig_w=fig_w, fig_h=fig_h*2)\n",
    "            self.plot_comparison2(x=self.x, y=self.y, \n",
    "                                 phi_pinn=u_pinn.reshape(self.n_y, self.n_x), \n",
    "                                 phi_analytics=u_test.reshape(self.n_y, self.n_x), \n",
    "                                 fig_w=fig_w, fig_h=fig_h*2)\n",
    "            v_max = max(u_test)\n",
    "            v_min = 0.0\n",
    "        elif self.model.bc_type == \"Neumann\":\n",
    "            self.neumann_loss()\n",
    "            v_max = int(max(self.u_pinn))\n",
    "            v_min = case.init_wall\n",
    "            fig_w = int(fig_w*2/3)\n",
    "\n",
    "        # PLOT: u_velocity\n",
    "        \n",
    "        self.plot_countour(x=self.X, y=self.Y, \n",
    "                           x_flat=self.X_flat, y_flat=self.Y_flat,\n",
    "                           phi=self.u_pinn, phi_=self.u_pinn_, \n",
    "                           vmin=v_min, vmax=v_max,\n",
    "                           types=\"T\", \n",
    "                           fig_w=fig_w, fig_h=fig_h)\n",
    "\n",
    "        # # PLOT: v_velocity\n",
    "        # self.plot_countour(x=self.X, y=self.Y, \n",
    "        #                    x_flat=self.X_flat, y_flat=self.Y_flat,\n",
    "        #                    phi=self.v_pinn, phi_=self.v_pinn_, \n",
    "        #                    vmin=-0.3, vmax=0.3,\n",
    "        #                    types=\"v\", \n",
    "        #                    fig_w=fig_w, fig_h=fig_h)\n",
    "        \n",
    "        # # PLOT: p_pressure\n",
    "        # self.plot_countour(x=self.X, y=self.Y, \n",
    "        #                    x_flat=self.X_flat, y_flat=self.Y_flat,\n",
    "        #                    phi=self.p_pinn, phi_=self.p_pinn_, \n",
    "        #                    vmin=0.0, vmax=5.0,\n",
    "        #                    types=\"p\", \n",
    "        #                    fig_w=fig_w, fig_h=fig_h)\n",
    "\n",
    "    def plot_comparison(self, x, y, phi_pinn, phi_analytics, fig_w, fig_h):\n",
    "        #plt.rc('text', usetex=True)\n",
    "        #plt.rc('font', family='serif')\n",
    "        # phi_pinn = phi_pinn.\n",
    "        # phi_analytics = phi_analytics.reshape(self.n_y, self.n_x)\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(fig_w, fig_h), dpi=100, constrained_layout=False)\n",
    "        \n",
    "        # print(phi_pinn)\n",
    "        ax[1].plot(y, phi_pinn[:,self.n_x//2].flatten(), 'b', label=\"PINN solution\", linewidth=4)\n",
    "        ax[1].plot(y, phi_analytics[:,self.n_x//2], '--r', label=\"Analytical solution\", linewidth=4)\n",
    "        \n",
    "\n",
    "        x_lim_min = min(np.min(phi_pinn), np.min(phi_analytics))\n",
    "        x_lim_max = max(np.max(phi_pinn), np.max(phi_analytics))\n",
    "        x_lim_min = 0\n",
    "        x_lim_max = x_lim_max + 0.2*abs(x_lim_max)\n",
    "        ax[1].set_ylim(x_lim_min, x_lim_max)\n",
    "        ax[1].set_xlim(self.lb[1], self.ub[1])\n",
    "        #ax.set_xlabel(\"$u$ (m/s)\", fontsize=20)\n",
    "        #ax.set_ylabel(\"$y$ (m)\", fontsize=20)\n",
    "        ax[1].set_xlabel(\"$t$ (s), $x=0.5$ m\", fontsize=15)\n",
    "        ax[1].set_ylabel(\"$T$ (\\u2103)\", fontsize=15)\n",
    "        ax[1].grid(linestyle=\"--\")\n",
    "        ax[1].legend(fontsize=10)\n",
    "        #ax.set_title(\"$u$-velocity comparison\", fontsize=20)\n",
    "        ax[1].set_title(\"Temperature comparison\", fontsize=20)\n",
    "        ax[1].tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "        ax[0].plot(y, phi_pinn[:,self.n_x//4].flatten(), 'b', label=\"PINN solution\", linewidth=4)\n",
    "        ax[0].plot(y, phi_analytics[:,self.n_x//4], '--r', label=\"Analytical solution\", linewidth=4)\n",
    "        \n",
    "        # x_lim_min = min(min(phi_pinn[:,1:2].flatten()), min(phi_analytics[:,1:2]))\n",
    "        # x_lim_max = max(max(phi_pinn[:,1:2].flatten()), max(phi_analytics[:,1:2]))\n",
    "        # x_lim_min = x_lim_min - 0.2*abs(x_lim_min)\n",
    "        # x_lim_max = x_lim_max + 0.2*abs(x_lim_max)\n",
    "        ax[0].set_ylim(x_lim_min, x_lim_max)\n",
    "        ax[0].set_xlim(self.lb[1], self.ub[1])\n",
    "        #ax.set_xlabel(\"$u$ (m/s)\", fontsize=20)\n",
    "        #ax.set_ylabel(\"$y$ (m)\", fontsize=20)\n",
    "        ax[0].set_xlabel(\"$t$ (s), $x=0.25$ m\", fontsize=15)\n",
    "        ax[0].set_ylabel(\"$T$ (\\u2103)\", fontsize=15)\n",
    "        ax[0].grid(linestyle=\"--\")\n",
    "        ax[0].legend(fontsize=10)\n",
    "        #ax.set_title(\"$u$-velocity comparison\", fontsize=20)\n",
    "        # ax[0].set_title(\"Temperature comparison\", fontsize=20)\n",
    "        ax[0].tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "        ax[2].plot(y, phi_pinn[:,self.n_x//4*3+1].flatten(), 'b', label=\"PINN solution\", linewidth=4)\n",
    "        ax[2].plot(y, phi_analytics[:,self.n_x//4*3+1], '--r', label=\"Analytical solution\", linewidth=4)\n",
    "        \n",
    "        # x_lim_min = min(min(phi_pinn[:,2:3].flatten()), min(phi_analytics[:,2:3]))\n",
    "        # x_lim_max = max(max(phi_pinn[:,2:3].flatten()), max(phi_analytics[:,2:3]))\n",
    "        # x_lim_min = x_lim_min - 0.2*abs(x_lim_min)\n",
    "        # x_lim_max = x_lim_max + 0.2*abs(x_lim_max)\n",
    "        ax[2].set_ylim(x_lim_min, x_lim_max)\n",
    "        ax[2].set_xlim(self.lb[1], self.ub[1])\n",
    "        #ax.set_xlabel(\"$u$ (m/s)\", fontsize=20)\n",
    "        #ax.set_ylabel(\"$y$ (m)\", fontsize=20)\n",
    "        ax[2].set_xlabel(\"$t$ (s), $x=0.75$ m\", fontsize=15)\n",
    "        ax[2].set_ylabel(\"$T$ (\\u2103)\", fontsize=15)\n",
    "        ax[2].grid(linestyle=\"--\")\n",
    "        ax[2].legend(fontsize=10)\n",
    "        #ax.set_title(\"$u$-velocity comparison\", fontsize=20)\n",
    "        # ax[2].set_title(\"Temperature comparison\", fontsize=20)\n",
    "        ax[2].tick_params(axis='both', which='major', labelsize=16)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    def plot_comparison2(self, x, y, phi_pinn, phi_analytics, fig_w, fig_h):\n",
    "        #plt.rc('text', usetex=True)\n",
    "        #plt.rc('font', family='serif')\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(fig_w, fig_h), dpi=100, constrained_layout=False)\n",
    "        \n",
    "        # print(phi_pinn)\n",
    "        ax[1].plot(x, phi_pinn[self.n_x//2,:].flatten(), 'b', label=\"PINN solution\", linewidth=4)\n",
    "        ax[1].plot(x, phi_analytics[self.n_x//2,:], '--r', label=\"Analytical solution\", linewidth=4)\n",
    "        \n",
    "\n",
    "        x_lim_min = min(np.min(phi_pinn), np.min(phi_analytics))\n",
    "        x_lim_max = max(np.max(phi_pinn), np.max(phi_analytics))\n",
    "        x_lim_min = 0\n",
    "        x_lim_max = x_lim_max + 0.2*abs(x_lim_max)\n",
    "        ax[1].set_ylim(x_lim_min, x_lim_max)\n",
    "        ax[1].set_xlim(self.lb[1], self.ub[1])\n",
    "        ax[1].set_xlabel(\"$x$ (m), $t=0.5$ s \", fontsize=18)\n",
    "        ax[1].set_ylabel(\"$T$ (\\u2103)\", fontsize=18)\n",
    "        ax[1].grid(linestyle=\"--\")\n",
    "        ax[1].legend(fontsize=10)\n",
    "        ax[1].set_title(\"Temperature comparison\", fontsize=25)\n",
    "        ax[1].tick_params(axis='both', which='major', labelsize=19)\n",
    "\n",
    "        ax[0].plot(x, phi_pinn[self.n_x//4,:].flatten(), 'b', label=\"PINN solution\", linewidth=4)\n",
    "        ax[0].plot(x, phi_analytics[self.n_x//4,:], '--r', label=\"Analytical solution\", linewidth=4)\n",
    "        \n",
    "        ax[0].set_ylim(x_lim_min, x_lim_max)\n",
    "        ax[0].set_xlim(self.lb[1], self.ub[1])\n",
    "        ax[0].set_xlabel(\"$x$ (m), $t=0.25$ s \", fontsize=18)\n",
    "        ax[0].set_ylabel(\"$T$ (\\u2103)\", fontsize=18)\n",
    "        ax[0].grid(linestyle=\"--\")\n",
    "        ax[0].legend(fontsize=10)\n",
    "        ax[0].tick_params(axis='both', which='major', labelsize=19)\n",
    "\n",
    "        ax[2].plot(x, phi_pinn[self.n_x//4*3+1,:].flatten(), 'b', label=\"PINN solution\", linewidth=4)\n",
    "        ax[2].plot(x, phi_analytics[self.n_x//4*3+1,:], '--r', label=\"Analytical solution\", linewidth=4)\n",
    "        \n",
    "        ax[2].set_ylim(x_lim_min, x_lim_max)\n",
    "        ax[2].set_xlim(self.lb[1], self.ub[1])\n",
    "        ax[2].set_xlabel(\"$x$ (m), $t=0.75$ s \", fontsize=18)\n",
    "        ax[2].set_ylabel(\"$T$ (\\u2103)\", fontsize=18)\n",
    "        ax[2].grid(linestyle=\"--\")\n",
    "        ax[2].legend(fontsize=10)\n",
    "        ax[2].tick_params(axis='both', which='major', labelsize=19)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    def plot_countour(self, x, y, x_flat, y_flat, phi, phi_, vmin, vmax, types, fig_w, fig_h):\n",
    "        # Set\n",
    "        #plt.rc('text', usetex=True)\n",
    "        #plt.rc('font', family='serif')\n",
    "        \n",
    "        # Create plot\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(fig_w, fig_h), dpi=300, constrained_layout=False)\n",
    "\n",
    "        # Unpack\n",
    "        cf = ax.scatter(x_flat, y_flat, c=phi, \n",
    "                        alpha=1., edgecolors='none', cmap='jet', marker=\".\", s=50, vmin=vmin, vmax=vmax)\n",
    "        ax.set_xlim(self.lb[0], self.ub[0])\n",
    "        ax.set_ylim(self.lb[1], self.ub[1])\n",
    "        \n",
    "        if types==\"p\":\n",
    "            #ax.set_title(f'${types}$ (Pa)', fontsize=20)\n",
    "            ax.set_title(f'{types} (Pa)', fontsize=20)\n",
    "        else:\n",
    "            #ax.set_title(f'${types}$ (m/s)', fontsize=20)\n",
    "            ax.set_title(f'${types} (x,t)$ PINN Solution', fontsize=20)\n",
    "        \n",
    "        #ax.set_xlabel('$x$ (m)', fontsize=20)\n",
    "        #ax.set_ylabel('$y$ (m)', fontsize=20)\n",
    "        ax.set_xlabel('$x$ (m)', fontsize=20)\n",
    "        ax.set_ylabel('$t$ (s)', fontsize=20)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "        ax.contour(x, y, phi_, colors='k', linewidths=0.2, levels=50)\n",
    "        \n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"2%\", pad=0.1)\n",
    "        cb = fig.colorbar(cf, cax=cax)\n",
    "        cb.ax.set_title('$T$(\\u2103)', fontsize=18)\n",
    "        ticks = np.linspace(vmin, vmax, 3)\n",
    "        ticks = np.floor(ticks)\n",
    "        cb.set_ticks(ticks)\n",
    "        cb.ax.tick_params(labelsize=16)\n",
    "\n",
    "        if self.model.bc_type == 'Dirichlet':\n",
    "          # Analytical Solution contour\n",
    "          T = np.exp(x_flat+2*y_flat)\n",
    "          fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(fig_w, fig_h), dpi=300, constrained_layout=False)\n",
    "          cf = ax.scatter(x_flat, y_flat, c=T, alpha=1., edgecolors='none', cmap='jet', marker=\".\", s=50, vmin=vmin, vmax=vmax)\n",
    "          ax.set_xlim(self.lb[0], self.ub[0])\n",
    "          ax.set_ylim(self.lb[1], self.ub[1])\n",
    "          \n",
    "          if types==\"p\":\n",
    "              #ax.set_title(f'${types}$ (Pa)', fontsize=20)\n",
    "              ax.set_title(f'{types} (Pa)', fontsize=20)\n",
    "          else:\n",
    "              #ax.set_title(f'${types}$ (m/s)', fontsize=20)\n",
    "              ax.set_title(f'${types} (x,t)$ Analytical Solution', fontsize=20)\n",
    "          \n",
    "          #ax.set_xlabel('$x$ (m)', fontsize=20)\n",
    "          #ax.set_ylabel('$y$ (m)', fontsize=20)\n",
    "          ax.set_xlabel('$x$ (m)', fontsize=20)\n",
    "          ax.set_ylabel('$t$ (s)', fontsize=20)\n",
    "          ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "          ax.contour(x, y, np.exp(x + 2*y), colors='k', linewidths=0.2, levels=50)\n",
    "          \n",
    "          divider = make_axes_locatable(ax)\n",
    "          cax = divider.append_axes(\"right\", size=\"2%\", pad=0.1)\n",
    "          cb = fig.colorbar(cf, cax=cax)\n",
    "          cb.ax.set_title('$T$(\\u2103)', fontsize=18)\n",
    "          ticks = np.linspace(vmin, vmax, 3)\n",
    "#           print(ticks)\n",
    "          ticks = np.floor(ticks)\n",
    "          cb.set_ticks(ticks)\n",
    "          cb.ax.tick_params(labelsize=16)\n",
    "\n",
    "          plt.show()\n",
    "\n",
    "          # fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(fig_w, fig_h), dpi=100, constrained_layout=False)\n",
    "          # df = pd.DataFrame({'x':x_flat, 't':y_flat, 'T':phi})\n",
    "          # df.iplot(kind='surface',colorscale='rdylbu')\n",
    "\n",
    "        \n",
    "\n",
    "        # save figure\n",
    "        if self.save_fig:\n",
    "            fig.savefig(f\"fig_{types}.eps\", format=\"eps\")\n",
    "        plt.show()\n",
    "\n",
    "    def unpack(self, model, params):\n",
    "        self.model = model\n",
    "        self.params = params\n",
    "\n",
    "        self.lb = params[\"data\"][\"lb\"]\n",
    "        self.ub = params[\"data\"][\"ub\"]\n",
    "        self.n_test = params[\"data\"][\"n_test\"]\n",
    "        # self.u_max = params[\"physic\"][\"u_max\"]\n",
    "        # self.u_wall = params[\"physic\"][\"u_wall\"]\n",
    "        # self.rho = params[\"physic\"][\"rho\"]\n",
    "        # self.mu = params[\"physic\"][\"mu\"]\n",
    "        self.epochs = params[\"network\"][\"epochs\"]\n",
    "        self.coef_bc = params[\"network\"][\"coef_bc\"]\n",
    "        self.verboses_adam = params[\"network\"][\"verboses_adam\"]\n",
    "        self.verboses_newton = params[\"network\"][\"verboses_newton\"]\n",
    "        self.verboses_adaptive = params[\"network\"][\"verboses_adaptive\"]\n",
    "    def neumann_loss(self):\n",
    "        # self.create_test_data()\n",
    "        x = self.model.x_test\n",
    "        y = self.model.y_test\n",
    "        u_pinn = self.model.predict(x, y)\n",
    "        if self.model.bc_type == 'Neumann':\n",
    "            u_fea = pd.read_csv('import_test_t2160.csv')\n",
    "            u_fea = u_fea.to_numpy()\n",
    "            # T_interp = scipy.interpolate.interp1d(u_fea[:,0],u_fea[:,1])\n",
    "            # u_test = []\n",
    "            # for i in self.X_flat:\n",
    "            #   u_test.append(T_interp(i))\n",
    "            # u_test = u_fea[:,1:2]+273.0\n",
    "            u_test = u_fea[:,1:2]\n",
    "\n",
    "        delta_u = np.abs(u_pinn - u_test)\n",
    "        #  Absolute error with n_x and n_y points\n",
    "        abs_err_u = np.sum(delta_u)/(delta_u.shape[0])\n",
    "\n",
    "        rel_err_u_ij = delta_u/u_test\n",
    "        # Relative error with n_x and n_y points\n",
    "        rel_err_u = np.sum(rel_err_u_ij)/(delta_u.shape[0])\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6), dpi=300, constrained_layout=False)\n",
    "        ax.plot(x, u_pinn, 'blue', label=\"PINN Solution\", linewidth=4)\n",
    "        ax.plot(u_fea[:,0] , u_test, '--r', label=\"Finite Element Solution\", linewidth=4)\n",
    "        # ax.plot(u_fea[:,0] , u_test, '--r')\n",
    "        ax.set_xlabel(\"x meter\", fontsize=15)\n",
    "        ax.set_ylabel(\"T (x, t = 2160s) Celcius\", fontsize=15)\n",
    "        ax.grid(linestyle=\"--\")\n",
    "        ax.legend(fontsize=15)\n",
    "        ax.set_title(\"Temperature Comparison\", fontsize=20)\n",
    "        ax.tick_params(axis=\"both\", which='major', labelsize=16)\n",
    "\n",
    "        # rmse_total = rmse_u\n",
    "        # print(f\"- RMSE x 10^(-2): {rmse_total/(1.e-2):5f}\")\n",
    "        print(f\"- Absolute Error: {abs_err_u:5f}\")\n",
    "        print(f\"- Relative Error (%): {rel_err_u*100:5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaq_LJNqZCAR"
   },
   "source": [
    "# **RUN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1245,
     "status": "ok",
     "timestamp": 1646374740530,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "c3WZht2MZX4N"
   },
   "outputs": [],
   "source": [
    "# %tensorflow_version 2.x\n",
    "# import tensorflow as tf\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#   raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "\n",
    "# Set GPU to tensorflow session\n",
    "with tf.device('/device:GPU:2'):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    # session = tf.InteractiveSession(config=config)\n",
    "    session = tf.Session(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p2TyP64ZHoZ"
   },
   "source": [
    "## **Channel Flow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1646374740531,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "GcYF_vFYZCyZ"
   },
   "outputs": [],
   "source": [
    "def material_prop(material):\n",
    "    if material == \"wood\":\n",
    "        # Density (rho) in kg/m^3\n",
    "        dry_rho = 300 \n",
    "        rho_ratio = np.array([[1.12],\n",
    "                              [1.12],\n",
    "                              [1.0],\n",
    "                              [1.0],\n",
    "                              [0.93]])\n",
    "        rho_temp = np.array([[20.0],\n",
    "                             [99.0],\n",
    "                             [120.0],\n",
    "                             [200.0],\n",
    "                             [250.0]])\n",
    "        rho_data = np.concatenate((rho_temp, dry_rho*rho_ratio), axis=1)\n",
    "\n",
    "        # Thermal conductivity (k) in W/mK\n",
    "        k_data = np.array([[0.12],\n",
    "                           [0.15]])\n",
    "        k_temp = np.array([[20.0],\n",
    "                           [200.0]])\n",
    "        k_data = np.concatenate((k_temp, k_data), axis=1)\n",
    "\n",
    "        # Specific Heat (cp) in J/kgK\n",
    "        cp_data = np.array([[1.53e+3],\n",
    "                            [1.77e+3],\n",
    "                            [13.6e+3],\n",
    "                            [13.5e+3],\n",
    "                            [2.12e+3],\n",
    "                            [2e+3],\n",
    "                            [1.62e+3]])\n",
    "        cp_temp = np.array([[20.0],\n",
    "                            [99.0],\n",
    "                            [99.001],\n",
    "                            [120.0],\n",
    "                            [120.001],\n",
    "                            [200.0],\n",
    "                            [250.0]])\n",
    "        cp_data = np.concatenate((cp_temp, cp_data), axis=1)\n",
    "    elif material == \"steel\":\n",
    "        pass\n",
    "    return rho_data, k_data, cp_data\n",
    "\n",
    "\n",
    "def generate_param():\n",
    "    rho, k, cp = material_prop(material=\"wood\")\n",
    "    params = {}\n",
    "\n",
    "    params[\"data\"] = {}\n",
    "    params[\"data\"][\"lb\"] = np.array([0., 0.])\n",
    "    # params[\"data\"][\"ub\"] = np.array([0.1, 3600.0])\n",
    "    params[\"data\"][\"ub\"] = np.array([1.0, 1.0])\n",
    "    # params[\"data\"][\"n_collo\"] = 8000\n",
    "    params[\"data\"][\"n_collo\"] = 1000\n",
    "    # params[\"data\"][\"n_inlet\"] = 301\n",
    "#     params[\"data\"][\"n_inlet\"] = 101\n",
    "    # params[\"data\"][\"n_outlet\"] = 301\n",
    "#     params[\"data\"][\"n_outlet\"] = 101\n",
    "    # params[\"data\"][\"n_wall\"] = 201\n",
    "    params[\"data\"][\"n_test\"] = 201\n",
    "    params[\"data\"][\"init_wall\"] = 25  # Neumann case (in K)\n",
    "    # params[\"data\"][\"init_wall\"] = 25 + 273# Neumann case (in K)\n",
    "    params[\"data\"][\"M\"] = 500\n",
    "    # params[\"data\"][\"M\"] = 3\n",
    "    params[\"data\"][\"left_q\"] = 0 # W/m^2\n",
    "    params[\"data\"][\"right_q\"] = 300 # W/m^2\n",
    "    params[\"data\"][\"t_check\"] = 2160\n",
    "    params[\"data\"][\"seed\"] = 543212\n",
    "    n = 101\n",
    "    params[\"data\"][\"n_wall\"] = n\n",
    "    loc_num = 3\n",
    "    params[\"data\"][\"loc\"] = np.linspace(0, 1, loc_num) #in %\n",
    "    params[\"data\"][\"meu\"] = [n]*loc_num\n",
    "    # 0 1 2 3 4 10 30 40 12345\n",
    "    \n",
    "\n",
    "    params[\"physic\"] = {}\n",
    "    params[\"physic\"][\"rho\"] = rho\n",
    "    params[\"physic\"][\"cp\"]  = cp\n",
    "    params[\"physic\"][\"k\"]   = k\n",
    "    # params[\"physic\"][\"rho\"] = 336  # kg/m^3\n",
    "    # params[\"physic\"][\"cp\"]  = 1530 # J/(kg.K)\n",
    "    # params[\"physic\"][\"k\"]   = 0.12 # W/(mK)\n",
    "    # params[\"physic\"][\"mu\"] = 0.02\n",
    "\n",
    "    # params[\"physic\"][\"T_left\"] = 50\n",
    "    # params[\"physic\"][\"T_right\"] = 100\n",
    "    # params[\"physic\"][\"T_lower\"] = 300\n",
    "    # params[\"physic\"][\"T_upper\"] = 150\n",
    "\n",
    "    params[\"network\"] = {}\n",
    "    params[\"network\"][\"epochs\"] = 501 #10001\n",
    "    params[\"network\"][\"coef_bc\"] = 1.0\n",
    "    params[\"network\"][\"lr_init\"] = 1.0e-3\n",
    "    params[\"network\"][\"beta\"] = 0.9\n",
    "    params[\"network\"][\"batches\"] = 2000\n",
    "    params[\"network\"][\"verboses_adam\"] = 100\n",
    "    params[\"network\"][\"verboses_newton\"] = 100\n",
    "    params[\"network\"][\"verboses_adaptive\"] = 10\n",
    "    params[\"network\"][\"saver\"] = 5000\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beNqkcfmZJtv"
   },
   "source": [
    "#### **Poiseuille Flow - VP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUYLHAeiaRmo"
   },
   "source": [
    "##### **Generate Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 831
    },
    "executionInfo": {
     "elapsed": 1739,
     "status": "ok",
     "timestamp": 1646216597614,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "fefxq0D_ZNst",
    "outputId": "7db83271-562a-4f4a-cae3-649bc6338ce3"
   },
   "outputs": [],
   "source": [
    "BC = 'Dirichlet'\n",
    "# BC = 'Neumann'\n",
    "# prob = 'Direct'\n",
    "prob = 'Inverse'\n",
    "\n",
    "\n",
    "params = generate_param()\n",
    "\n",
    "# fun = \"relu\"\n",
    "fun = \"tanh\"\n",
    "# fun = \"swish\"\n",
    "# fun = \"sigmoid\"\n",
    "params[\"network\"][\"act_fun\"] = fun\n",
    "# params[\"physic\"][\"u_max\"] = 2.0\n",
    "# params[\"physic\"][\"u_wall\"] = 0.0\n",
    "# params[\"network\"][\"layers\"] = [2] + 4*[50] + [3]\n",
    "\n",
    "\n",
    "# params[\"network\"][\"layers\"] = [2] + 8*[40] + [1]\n",
    "# params[\"network\"][\"layers\"] = [2] + 8*[15] + [1]\n",
    "\n",
    "# best config\n",
    "# params[\"network\"][\"layers\"] = [2] + 4*[40] + [1]\n",
    "# inverse\n",
    "# params[\"network\"][\"layers\"] = [2] + 4*[40] + [2]\n",
    "params[\"network\"][\"layers\"] = [2] + 2*[5] + [1]\n",
    "params[\"data\"][\"noise\"] = 0.03\n",
    "\n",
    "case = CasesChannel(add_noise=True)\n",
    "# case.generate_train_data(param=params, bc_type='Neumann')\n",
    "case.generate_train_data(param=params, bc_type=BC)\n",
    "case.generate_test_data(param=params)\n",
    "case.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,1,101)\n",
    "y = np.linspace(0,1,101)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "x_flat = X.flatten()[:,None]\n",
    "y_flat = Y.flatten()[:,None]\n",
    "\n",
    "x = x_flat\n",
    "t = y_flat\n",
    "h = np.exp(x + 2*t)\n",
    "\n",
    "fig_h = 3\n",
    "fig_w = 5\n",
    "\n",
    "vmin=min(h)\n",
    "vmax=max(h)\n",
    "        \n",
    "# results.plot_countour(x=X, y=Y, \n",
    "#                       x_flat=X_flat, y_flat=Y_flat,\n",
    "#                       phi=h, phi_=h.reshape(101, 101),\n",
    "#                       vmin=min(h), vmax=max(h),\n",
    "#                       fig_w=fig_w, fig_h=fig_h)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(fig_w, fig_h), dpi=300, constrained_layout=False)\n",
    "\n",
    "# Unpack\n",
    "cf = ax.scatter(x_flat, y_flat, c=h, \n",
    "                alpha=1., edgecolors='none', cmap='jet', marker=\".\", s=50, vmin=vmin, vmax=vmax)\n",
    "# ax.set_xlim(self.lb[0], self.ub[0])\n",
    "# ax.set_ylim(self.lb[1], self.ub[1])\n",
    "\n",
    "ax.set_title(f'Heat Generation', fontsize=25)\n",
    "\n",
    "ax.set_xlabel('$x$ (m)', fontsize=20)\n",
    "ax.set_ylabel('$t$ (s)', fontsize=20)\n",
    "#         ax.set_xlabel('x', fontsize=20)\n",
    "#         ax.set_ylabel('t', fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax.contour(X, Y, h.reshape(101,101), colors='k', linewidths=0.2, levels=50)\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"2%\", pad=0.1)\n",
    "cb = fig.colorbar(cf, cax=cax)\n",
    "ticks = np.linspace(vmin, vmax, 3)\n",
    "ticks = np.round(ticks, 2)\n",
    "ticks[0] = ticks[0] + 0.01\n",
    "cb.set_ticks(ticks)\n",
    "cb.ax.set_title('$q$[W/m3]',fontsize=15)\n",
    "cb.ax.tick_params(labelsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJOUZyXRZgwz"
   },
   "source": [
    "##### **Run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15213,
     "status": "ok",
     "timestamp": 1646216612796,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "pdRbx7ICZiD3",
    "outputId": "87825508-9007-47b3-cf08-6a509ae86c37"
   },
   "outputs": [],
   "source": [
    "# Create Model Instance\n",
    "model = PinnVP(data=case.data, adaptive_model=False,\n",
    "               params=params, bc_type=BC, problem_type=prob)\n",
    "              #  exist_model=True, file_dir='/content/poiseuille-vp.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hObAm2Nlyr4"
   },
   "outputs": [],
   "source": [
    "# Fit using ADAM\n",
    "model.fit_newton()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13797,
     "status": "ok",
     "timestamp": 1646216626584,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "AnoNB2RDlz9C",
    "outputId": "9d7c73bd-dc80-4f16-aa4b-8fb229998814"
   },
   "outputs": [],
   "source": [
    "lambda_log = np.array(model.lambda_1_log)[:,0]\n",
    "x1 = np.linspace(1, len(lambda_log), len(lambda_log))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6), constrained_layout=True, dpi=300)\n",
    "ax.plot(x1, lambda_log, \"r\", linestyle=\"solid\", label=\"Lambda\")\n",
    "# ax.plot(x2, lambda_2_log, \"g\", linestyle=\"dashdot\", label=\"Lambda 2\")\n",
    "\n",
    "ax.set_xlim(0, x1[-1])\n",
    "ax.set_xlabel(\"Iterations\", fontsize=12)\n",
    "ax.set_ylabel(\"Lambda Value\", fontsize=12)\n",
    "ax.grid(linestyle=\"--\")\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_title('Lambda History', fontsize=17)\n",
    "ax.tick_params(axis='both', which='major', labelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kh_tlpJZO6fb"
   },
   "outputs": [],
   "source": [
    "# tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9023,
     "status": "ok",
     "timestamp": 1646216635596,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "LZB-gi3Sl3xt",
    "outputId": "9c0a4c55-e2a5-433f-c779-40e5b0aae88d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Results\n",
    "results = PostChannel(model=model,\n",
    "                      params=params,\n",
    "                      save_fig=False)\n",
    "\n",
    "results.display_loss()\n",
    "results.display_contour()\n",
    "\n",
    "# Save model\n",
    "# model.save_model('test-seed=1-after.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ou_7SGmZ2ss3"
   },
   "outputs": [],
   "source": [
    "x, y, u_pinn, u_test = results.calculate_error(results.n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "executionInfo": {
     "elapsed": 492,
     "status": "error",
     "timestamp": 1646219549435,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "s31s-f4S4hZq",
    "outputId": "ed5d62b1-d50d-4b93-b91d-9b8b22259257"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_model('1_inverse_param.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCJqg2IDTqz-"
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "x = np.ones((n,1))*1\n",
    "t = lhs(1,n)*1\n",
    "# T = np.exp(1+2*(t/self.ub[1]))\n",
    "T = np.exp(x+2*t)\n",
    "\n",
    "mu = T\n",
    "noise = 0.10\n",
    "sigma = noise * mu\n",
    "np.random.seed(54321)\n",
    "T_new = np.random.normal(mu, sigma).reshape(mu.shape)\n",
    "T_neww = T + np.random.normal(0, noise*max(T),n).reshape(T.shape)\n",
    "T_newww = T + noise*np.std(T)*np.random.randn(T.shape[0],T.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1646238052702,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "kMffJzNibdzz",
    "outputId": "e4bb0519-e6d0-4607-b399-6b9f27140608"
   },
   "outputs": [],
   "source": [
    "print(np.mean(abs(T-T_new)))\n",
    "print(np.mean(abs(T-T_neww)))\n",
    "print(np.mean(abs(T-T_newww)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646238080853,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "yQQtGwMMv-t8",
    "outputId": "a0f8f17e-603d-4768-deba-563601a54144"
   },
   "outputs": [],
   "source": [
    "print(np.std(abs(T-T_new)))\n",
    "print(np.std(abs(T-T_neww)))\n",
    "print(np.std(abs(T-T_newww)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646238138758,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "3Ve3JDhbb8pH",
    "outputId": "ee21bb32-94b7-4672-e721-397f8a438375"
   },
   "outputs": [],
   "source": [
    "print(np.std(abs(T-T_new)) / np.mean(abs(T-T_new)))\n",
    "print(np.std(abs(T-T_neww)) / np.mean(abs(T-T_neww)))\n",
    "print(np.std(abs(T-T_newww)) / np.mean(abs(T-T_newww)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 191820,
     "status": "ok",
     "timestamp": 1646374932347,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "MSbSHn1ERPF0",
    "outputId": "937211c8-8506-47c4-ff27-221774b373d7"
   },
   "outputs": [],
   "source": [
    "# def main_loop(layers, neurons, noise, BC, prob, seed):\n",
    "#     t = timeit.default_timer()\n",
    "#     print(noise)\n",
    "#     params = generate_param()\n",
    "#     print(noise)\n",
    "#     params[\"network\"][\"layers\"] = [2] + layers*[neurons] + [1]\n",
    "#     params[\"data\"][\"noise\"] = noise\n",
    "#     params[\"data\"][\"seed\"] = seed\n",
    "\n",
    "#     # case = CasesChannel()\n",
    "#     case = CasesChannel(add_noise=True)\n",
    "#     case.generate_train_data(param=params, bc_type=BC)\n",
    "#     case.generate_test_data(param=params)\n",
    "\n",
    "#     model = PinnVP(data=case.data, adaptive_model=False,\n",
    "#                  params=params, bc_type=BC, problem_type=prob)\n",
    "\n",
    "#     model.fit_newton()\n",
    "\n",
    "#     results = PostChannel(model=model,\n",
    "#                       params=params,\n",
    "#                       save_fig=False)\n",
    "\n",
    "#     results.display_loss()\n",
    "#     #   case\n",
    "#     results.display_contour()\n",
    "#     elapsed = timeit.default_timer() - t\n",
    "\n",
    "#     if prob == \"Direct\":\n",
    "#         return results.abs_err_u, results.rel_err_u\n",
    "#     else:\n",
    "#         return model.lambda_1_log[-1][0][0], elapsed\n",
    "\n",
    "\n",
    "# # RUN\n",
    "# if __name__ == \"__main__\": \n",
    "#     layers = [2] # num of hidden layers\n",
    "#     neurons = [5] # num of neurons\n",
    "#     noises = [0.00, 0.01, 0.03] #, 0.15] # noise level\n",
    "#     seed = [543212345, 2345, 5432123, 543212, 54321, 345, 543, 54, 5, 4] # num of hidden layers\n",
    "#     # N_u = [500, 1000, 1500, 2000] # num of training data \n",
    "\n",
    "#     abs_err_table = np.zeros((len(layers), len(neurons)))\n",
    "#     rel_err_table = np.zeros((len(layers), len(neurons)))\n",
    "\n",
    "#     lambda_1_table1 = np.zeros((len(noises),len(seed)))\n",
    "#     lambda_1_table2 = np.zeros(len(noises))\n",
    "#     time_table = np.zeros((len(noises), len(seed)))\n",
    "\n",
    "#     # for i in range(len(layers)):\n",
    "#     #   for j in range(len(neurons)):\n",
    "#     #       abs_err_table[i,j], rel_err_table[i,j] = main_loop(layers[i], neurons[j], noises[0], BC=\"Dirichlet\", prob=\"Direct\")\n",
    "\n",
    "#     # np.savetxt('./abs_err_table.csv', abs_err_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "#     # np.savetxt('./rel_err_table.csv', rel_err_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "\n",
    "# #     for i in range(len(noises)):\n",
    "#     for j in range(0, len(seed)):\n",
    "#         lambda_1_table1[i, j], time_table[i,j] =  main_loop(2, 5, noises[0], BC=\"Dirichlet\", prob=\"Inverse\", seed=seed[j])\n",
    "#     #     lambda_1_table2[i] =  main_loop(6, 15, noises[i], BC=\"Dirichlet\", prob=\"Inverse\")\n",
    "\n",
    "#     np.savetxt('./lambda_1_table1.csv', lambda_1_table1, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "#     np.savetxt('./time_table.csv', time_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "\n",
    "#     #   np.savetxt('./lambda_1_table2.csv', lambda_1_table2, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1646374932347,
     "user": {
      "displayName": "Muhamad Abdul Aziz",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiHILWjr4ESArIjDWti8SP0Eoc2OKYZ3FHWCH5XZg=s64",
      "userId": "13788538733952970869"
     },
     "user_tz": -420
    },
    "id": "Sp9VuP8nQfg3",
    "outputId": "3f3ac607-7ec4-4919-83da-33351f3093bc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# relu\n",
    "def main_loop(layers, neurons, noise, BC, prob, seed):\n",
    "    t = timeit.default_timer()\n",
    "    print(noise)\n",
    "    params = generate_param()\n",
    "    print(seed)\n",
    "    params[\"network\"][\"layers\"] = [2] + layers*[neurons] + [1]\n",
    "    params[\"data\"][\"noise\"] = noise\n",
    "    params[\"data\"][\"seed\"] = seed\n",
    "    loc_num = 4\n",
    "    n = 101\n",
    "    params[\"data\"][\"n_wall\"] = n\n",
    "    params[\"data\"][\"loc\"] = np.linspace(0, 1, loc_num) #in %\n",
    "    params[\"data\"][\"meu\"] = [n]*loc_num\n",
    "    \n",
    "#     fun = \"relu\"\n",
    "    fun = \"tanh\"\n",
    "    # fun = \"swish\"\n",
    "    # fun = \"sigmoid\"\n",
    "    params[\"network\"][\"act_fun\"] = fun\n",
    "\n",
    "    # case = CasesChannel()\n",
    "    case = CasesChannel(add_noise=True)\n",
    "    case.generate_train_data(param=params, bc_type=BC)\n",
    "    case.generate_test_data(param=params)\n",
    "    case.plot()\n",
    "\n",
    "    model = PinnVP(data=case.data, adaptive_model=False,\n",
    "                 params=params, bc_type=BC, problem_type=prob)\n",
    "\n",
    "    model.fit_newton()\n",
    "    model_list.append(model)\n",
    "    \n",
    "\n",
    "    results = PostChannel(model=model,\n",
    "                      params=params,\n",
    "                      save_fig=False)\n",
    "\n",
    "    results.display_loss()\n",
    "    results_list.append(results)\n",
    "    \n",
    "#     lambda_log = []\n",
    "    lambda_log = (np.array(model.lambda_1_log)[:,0])\n",
    "    x = np.linspace(1, len(lambda_log), len(lambda_log))\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6), constrained_layout=True, dpi=300)\n",
    "    ax.plot(x, lambda_log, \"r\", linestyle=\"solid\", label=\"Noise free\")\n",
    "    ax.set_xlabel(\"Iterations\", fontsize=12)\n",
    "    ax.set_ylabel(\"Lambda Value\", fontsize=12)\n",
    "    ax.grid(linestyle=\"--\")\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_title('Lambda History', fontsize=17)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "\n",
    "    #   case\n",
    "    results.display_contour()\n",
    "    elapsed = timeit.default_timer() - t\n",
    "\n",
    "    if prob == \"Direct\":\n",
    "        return results.abs_err_u, results.rel_err_u\n",
    "    else:\n",
    "        return model.lambda_1_log[-1][0][0], elapsed\n",
    "\n",
    "\n",
    "# RUN\n",
    "if __name__ == \"__main__\": \n",
    "    layers = [2] # num of hidden layers\n",
    "    neurons = [5] # num of neurons\n",
    "    noises = [0.00, 0.01, 0.03] #, 0.15] # noise level\n",
    "    seed = [4]*3 #,2345, 5432123, 543212, 54321, 345, 543, 54, 5, 4] # num of hidden layers\n",
    "    # N_u = [500, 1000, 1500, 2000] # num of training data \n",
    "\n",
    "#     abs_err_table = np.zeros((len(layers), len(neurons)))\n",
    "#     rel_err_table = np.zeros((len(layers), len(neurons)))\n",
    "\n",
    "    lambda_1_table1 = np.zeros((len(noises),len(seed)))\n",
    "#     lambda_1_table2 = np.zeros(len(noises))\n",
    "    time_table = np.zeros((len(noises),len(seed)))\n",
    "    model_list = []\n",
    "    results_list = []\n",
    "\n",
    "    # for i in range(len(layers)):\n",
    "    #   for j in range(len(neurons)):\n",
    "    #       abs_err_table[i,j], rel_err_table[i,j] = main_loop(layers[i], neurons[j], noises[0], BC=\"Dirichlet\", prob=\"Direct\")\n",
    "\n",
    "    # np.savetxt('./abs_err_table.csv', abs_err_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "    # np.savetxt('./rel_err_table.csv', rel_err_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "\n",
    "    for i in range(len(noises)):\n",
    "#         for j in range(0, len(seed)):\n",
    "            lambda_1_table1[i, 0], time_table[i, 0] =  main_loop(2, 5, noises[i], BC=\"Dirichlet\", \n",
    "                                                                 prob=\"Inverse\", seed=seed[i])\n",
    "    #     lambda_1_table2[i] =  main_loop(6, 15, noises[i], BC=\"Dirichlet\", prob=\"Inverse\")\n",
    "    \n",
    "    \n",
    "\n",
    "#     np.savetxt('./lambda_1_table1_ReLU.csv', lambda_1_table1, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "#     np.savetxt('./time_table_ReLU.csv', time_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "    \n",
    "#     lambda_1_log = []\n",
    "#     for i in range(0, len(model_list)):\n",
    "#         model_list[i].save_model(f\"noise_0%_seed_{seed[i]}\")\n",
    "#         lambda_1_log.append(np.array(model_list[i].lambda_1_log)[:,0].flatten()[:,None])\n",
    "    \n",
    "#         np.savetxt(f'./lambda_1_log_noise_0%_seed_{seed[i]}.csv', lambda_1_log, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "        \n",
    "#     for i in range(len(lambda_log)):\n",
    "\n",
    "       \n",
    "\n",
    "    #   np.savetxt('./lambda_1_table2.csv', lambda_1_table2, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_pred_tanh = results_list[0].u_pinn.reshape(201,201)\n",
    "lambda_ref_tanh = results_list[0].u_test.reshape(201,201)\n",
    "\n",
    "lambda_pred_relu = results_list[1].u_pinn.reshape(201,201)\n",
    "lambda_ref_relu = results_list[1].u_test.reshape(201,201)\n",
    "\n",
    "lambda_pred_swish = results_list[2].u_pinn.reshape(201,201)\n",
    "lambda_ref_swish = results_list[2].u_test.reshape(201,201)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_w = 10\n",
    "fig_h = 6\n",
    "\n",
    "y = np.linspace(0, 1, 201)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(fig_w, fig_h), dpi=100, constrained_layout=False)\n",
    "        \n",
    "# print(phi_pinn)\n",
    "ax[0].plot(y, lambda_pred_tanh[:,100].flatten(), 'b', label=\"PINN solution\", linewidth=4)\n",
    "# ax[1].plot(y, phi_analytics[:,self.n_x//2], '--r', label=\"Analytical solution\", linewidth=4)\n",
    "ax[0].plot(y, lambda_ref_tanh[:,100].flatten(), '--r', label=\"Exact solution\", linewidth=4)\n",
    "\n",
    "\n",
    "x_lim_min = min(np.min(lambda_pred_tanh), np.min(lambda_ref_tanh))\n",
    "x_lim_max = max(np.max(lambda_pred_tanh), np.max(lambda_ref_tanh))\n",
    "# x_lim_min = 0\n",
    "# x_lim_max = x_lim_max + 0.2*abs(x_lim_max)\n",
    "ax[0].set_ylim(x_lim_min, x_lim_max)\n",
    "ax[0].set_xlim(0., 1.0)\n",
    "\n",
    "#ax.set_xlabel(\"$u$ (m/s)\", fontsize=20)\n",
    "#ax.set_ylabel(\"$y$ (m)\", fontsize=20)\n",
    "ax[0].set_xlabel(\"$t$ (s), $x=0.5$ m\", fontsize=25)\n",
    "# ax[1].set_ylabel(\"T (x = 0.5, t)\", fontsize=15)\n",
    "ax[0].set_ylabel(\"$T$ (\\u2103)\", fontsize=25)\n",
    "ax[0].grid(linestyle=\"--\")\n",
    "ax[0].legend(fontsize=14)\n",
    "ax[0].set_title(\"0\\% Noise\", fontsize=35)\n",
    "# ax[1].set_title(\"Temperature comparison\", fontsize=20)\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=23)\n",
    "\n",
    "ax[1].plot(y, lambda_pred_relu[:,100].flatten(), 'b', label=\"PINN solution\", linewidth=4)\n",
    "# ax[0].plot(y, phi_analytics[:,self.n_x//4], '--r', label=\"Analytical solution\", linewidth=4)\n",
    "ax[1].plot(y, lambda_ref_relu[:,100].flatten(), '--r', label=\"Exact solution\", linewidth=4)\n",
    "\n",
    "# x_lim_min = min(min(phi_pinn[:,1:2].flatten()), min(phi_analytics[:,1:2]))\n",
    "# x_lim_max = max(max(phi_pinn[:,1:2].flatten()), max(phi_analytics[:,1:2]))\n",
    "# x_lim_min = x_lim_min - 0.2*abs(x_lim_min)\n",
    "# x_lim_max = x_lim_max + 0.2*abs(x_lim_max)\n",
    "# ax[0].set_ylim(x_lim_min, x_lim_max)\n",
    "# ax[0].set_xlim(self.lb[1], self.ub[1])\n",
    "\n",
    "x_lim_min = min(np.min(lambda_pred_relu), np.min(lambda_ref_relu))\n",
    "x_lim_max = max(np.max(lambda_pred_relu), np.max(lambda_ref_relu))\n",
    "# x_lim_min = 0\n",
    "# x_lim_max = x_lim_max + 0.2*abs(x_lim_max)\n",
    "ax[1].set_ylim(x_lim_min, x_lim_max)\n",
    "ax[1].set_xlim(0., 1.0)\n",
    "#ax.set_xlabel(\"$u$ (m/s)\", fontsize=20)\n",
    "#ax.set_ylabel(\"$y$ (m)\", fontsize=20)\n",
    "ax[1].set_xlabel(\"$t$ (s), $x=0.5$ m\", fontsize=25)\n",
    "# ax[1].set_ylabel(\"T (x = 0.5, t)\", fontsize=15)\n",
    "ax[1].set_ylabel(\"$T$ (\\u2103)\", fontsize=25)\n",
    "ax[1].grid(linestyle=\"--\")\n",
    "ax[1].legend(fontsize=14)\n",
    "ax[1].set_title(\"1\\% Noise\", fontsize=35)\n",
    "# ax[0].set_title(\"Temperature comparison\", fontsize=20)\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=23)\n",
    "\n",
    "ax[2].plot(y, lambda_pred_swish[:,100].flatten(), 'b', label=\"PINN solution\", linewidth=4)\n",
    "# ax[2].plot(y, phi_analytics[:,self.n_x//4*3+1], '--r', label=\"Analytical solution\", linewidth=4)\n",
    "ax[2].plot(y, lambda_ref_swish[:,100].flatten(), '--r', label=\"Exact solution\", linewidth=4)\n",
    "\n",
    "# x_lim_min = min(min(phi_pinn[:,2:3].flatten()), min(phi_analytics[:,2:3]))\n",
    "# x_lim_max = max(max(phi_pinn[:,2:3].flatten()), max(phi_analytics[:,2:3]))\n",
    "# x_lim_min = x_lim_min - 0.2*abs(x_lim_min)\n",
    "# x_lim_max = x_lim_max + 0.2*abs(x_lim_max)\n",
    "# ax[2].set_ylim(x_lim_min, x_lim_max)\n",
    "# ax[2].set_xlim(self.lb[1], self.ub[1])\n",
    "\n",
    "x_lim_min = min(np.min(lambda_pred_swish), np.min(lambda_ref_swish))\n",
    "x_lim_max = max(np.max(lambda_pred_swish), np.max(lambda_ref_swish))\n",
    "# x_lim_min = 0\n",
    "# x_lim_max = x_lim_max + 0.2*abs(x_lim_max)\n",
    "ax[2].set_ylim(x_lim_min, x_lim_max)\n",
    "ax[2].set_xlim(0., 1.0)\n",
    "#ax.set_xlabel(\"$u$ (m/s)\", fontsize=20)\n",
    "#ax.set_ylabel(\"$y$ (m)\", fontsize=20)\n",
    "ax[2].set_xlabel(\"$t$ (s), $x=0.5$ m\", fontsize=25)\n",
    "# ax[1].set_ylabel(\"T (x = 0.5, t)\", fontsize=15)\n",
    "ax[2].set_ylabel(\"$T$ (\\u2103)\", fontsize=25)\n",
    "ax[2].grid(linestyle=\"--\")\n",
    "ax[2].legend(fontsize=14)\n",
    "ax[2].set_title(\"3\\% Noise\", fontsize=35)\n",
    "# ax[2].set_title(\"Temperature comparison\", fontsize=20)\n",
    "ax[2].tick_params(axis='both', which='major', labelsize=23)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### fig_w = 10\n",
    "fig_h = 6\n",
    "\n",
    "y = np.linspace(0, 1, 201)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(fig_w, fig_h), dpi=100, constrained_layout=False)\n",
    "        \n",
    "# print(phi_pinn)\n",
    "ax[0].plot(y, lambda_pred_tanh[100,:].flatten(), 'b', label=\"PINN solution\", linewidth=4)\n",
    "# ax[1].plot(y, phi_analytics[:,self.n_x//2], '--r', label=\"Analytical solution\", linewidth=4)\n",
    "ax[0].plot(y, lambda_ref_tanh[100,:].flatten(), '--r', label=\"Exact solution\", linewidth=4)\n",
    "\n",
    "\n",
    "x_lim_min = min(np.min(lambda_pred_tanh), np.min(lambda_ref_tanh))\n",
    "x_lim_max = max(np.max(lambda_pred_tanh), np.max(lambda_ref_tanh))\n",
    "# x_lim_min = 0\n",
    "# x_lim_max = x_lim_max + 0.2*abs(x_lim_max)\n",
    "ax[0].set_ylim(x_lim_min, x_lim_max)\n",
    "ax[0].set_xlim(0., 1.0)\n",
    "\n",
    "#ax.set_xlabel(\"$u$ (m/s)\", fontsize=20)\n",
    "#ax.set_ylabel(\"$y$ (m)\", fontsize=20)\n",
    "ax[0].set_xlabel(\"$x$ (m), $t=0.5$ s\", fontsize=25)\n",
    "# ax[1].set_ylabel(\"T (x = 0.5, t)\", fontsize=15)\n",
    "ax[0].set_ylabel(\"$T$ (\\u2103)\", fontsize=25)\n",
    "ax[0].grid(linestyle=\"--\")\n",
    "ax[0].legend(fontsize=14)\n",
    "ax[0].set_title(\"0\\% Noise\", fontsize=35)\n",
    "# ax[1].set_title(\"Temperature comparison\", fontsize=20)\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=23)\n",
    "\n",
    "ax[1].plot(y, lambda_pred_relu[100,:].flatten(), 'b', label=\"PINN solution\", linewidth=4)\n",
    "# ax[0].plot(y, phi_analytics[:,self.n_x//4], '--r', label=\"Analytical solution\", linewidth=4)\n",
    "ax[1].plot(y, lambda_ref_relu[100,:].flatten(), '--r', label=\"Exact solution\", linewidth=4)\n",
    "\n",
    "# x_lim_min = min(min(phi_pinn[:,1:2].flatten()), min(phi_analytics[:,1:2]))\n",
    "# x_lim_max = max(max(phi_pinn[:,1:2].flatten()), max(phi_analytics[:,1:2]))\n",
    "# x_lim_min = x_lim_min - 0.2*abs(x_lim_min)\n",
    "# x_lim_max = x_lim_max + 0.2*abs(x_lim_max)\n",
    "# ax[0].set_ylim(x_lim_min, x_lim_max)\n",
    "# ax[0].set_xlim(self.lb[1], self.ub[1])\n",
    "\n",
    "x_lim_min = min(np.min(lambda_pred_relu), np.min(lambda_ref_relu))\n",
    "x_lim_max = max(np.max(lambda_pred_relu), np.max(lambda_ref_relu))\n",
    "# x_lim_min = 0\n",
    "# x_lim_max = x_lim_max + 0.2*abs(x_lim_max)\n",
    "ax[1].set_ylim(x_lim_min, x_lim_max)\n",
    "ax[1].set_xlim(0., 1.0)\n",
    "#ax.set_xlabel(\"$u$ (m/s)\", fontsize=20)\n",
    "#ax.set_ylabel(\"$y$ (m)\", fontsize=20)\n",
    "ax[1].set_xlabel(\"$x$ (m), $t=0.5$ s\", fontsize=25)\n",
    "# ax[1].set_ylabel(\"T (x = 0.5, t)\", fontsize=15)\n",
    "ax[1].set_ylabel(\"$T$ (\\u2103)\", fontsize=25)\n",
    "ax[1].grid(linestyle=\"--\")\n",
    "ax[1].legend(fontsize=14)\n",
    "ax[1].set_title(\"1\\% Noise\", fontsize=35)\n",
    "# ax[0].set_title(\"Temperature comparison\", fontsize=20)\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=23)\n",
    "\n",
    "ax[2].plot(y, lambda_pred_swish[100,:].flatten(), 'b', label=\"PINN solution\", linewidth=4)\n",
    "# ax[2].plot(y, phi_analytics[:,self.n_x//4*3+1], '--r', label=\"Analytical solution\", linewidth=4)\n",
    "ax[2].plot(y, lambda_ref_swish[100,:].flatten(), '--r', label=\"Exact solution\", linewidth=4)\n",
    "\n",
    "# x_lim_min = min(min(phi_pinn[:,2:3].flatten()), min(phi_analytics[:,2:3]))\n",
    "# x_lim_max = max(max(phi_pinn[:,2:3].flatten()), max(phi_analytics[:,2:3]))\n",
    "# x_lim_min = x_lim_min - 0.2*abs(x_lim_min)\n",
    "# x_lim_max = x_lim_max + 0.2*abs(x_lim_max)\n",
    "# ax[2].set_ylim(x_lim_min, x_lim_max)\n",
    "# ax[2].set_xlim(self.lb[1], self.ub[1])\n",
    "\n",
    "x_lim_min = min(np.min(lambda_pred_swish), np.min(lambda_ref_swish))\n",
    "x_lim_max = max(np.max(lambda_pred_swish), np.max(lambda_ref_swish))\n",
    "# x_lim_min = 0\n",
    "# x_lim_max = x_lim_max + 0.2*abs(x_lim_max)\n",
    "ax[2].set_ylim(x_lim_min, x_lim_max)\n",
    "ax[2].set_xlim(0., 1.0)\n",
    "#ax.set_xlabel(\"$u$ (m/s)\", fontsize=20)\n",
    "#ax.set_ylabel(\"$y$ (m)\", fontsize=20)\n",
    "ax[2].set_xlabel(\"$x$ (m), $t=0.5$ s\", fontsize=25)\n",
    "# ax[1].set_ylabel(\"T (x = 0.5, t)\", fontsize=15)\n",
    "ax[2].set_ylabel(\"$T$ (\\u2103)\", fontsize=25)\n",
    "ax[2].grid(linestyle=\"--\")\n",
    "ax[2].legend(fontsize=14)\n",
    "ax[2].set_title(\"3\\% Noise\", fontsize=35)\n",
    "# ax[2].set_title(\"Temperature comparison\", fontsize=20)\n",
    "ax[2].tick_params(axis='both', which='major', labelsize=23)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# swish\n",
    "def main_loop(layers, neurons, noise, BC, prob, seed):\n",
    "    t = timeit.default_timer()\n",
    "    print(noise)\n",
    "    params = generate_param()\n",
    "    print(seed)\n",
    "    params[\"network\"][\"layers\"] = [2] + layers*[neurons] + [1]\n",
    "    params[\"data\"][\"noise\"] = noise\n",
    "    params[\"data\"][\"seed\"] = seed\n",
    "    loc_num = 4\n",
    "    n = 101\n",
    "    params[\"data\"][\"n_wall\"] = n\n",
    "    params[\"data\"][\"loc\"] = np.linspace(0, 1, loc_num) #in %\n",
    "    params[\"data\"][\"meu\"] = [n]*loc_num\n",
    "    \n",
    "#     fun = \"relu\"\n",
    "#     fun = \"tanh\"\n",
    "    fun = \"swish\"\n",
    "    # fun = \"sigmoid\"\n",
    "    params[\"network\"][\"act_fun\"] = fun\n",
    "\n",
    "    # case = CasesChannel()\n",
    "    case = CasesChannel(add_noise=True)\n",
    "    case.generate_train_data(param=params, bc_type=BC)\n",
    "    case.generate_test_data(param=params)\n",
    "    case.plot()\n",
    "\n",
    "    model = PinnVP(data=case.data, adaptive_model=False,\n",
    "                 params=params, bc_type=BC, problem_type=prob)\n",
    "\n",
    "    model.fit_newton()\n",
    "    model_list.append(model)\n",
    "    \n",
    "\n",
    "    results = PostChannel(model=model,\n",
    "                      params=params,\n",
    "                      save_fig=False)\n",
    "\n",
    "    results.display_loss()\n",
    "    \n",
    "#     lambda_log = []\n",
    "    lambda_log = (np.array(model.lambda_1_log)[:,0])\n",
    "    x = np.linspace(1, len(lambda_log), len(lambda_log))\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6), constrained_layout=True, dpi=300)\n",
    "    ax.plot(x, lambda_log, \"r\", linestyle=\"solid\", label=\"Noise free\")\n",
    "    ax.set_xlabel(\"Iterations\", fontsize=12)\n",
    "    ax.set_ylabel(\"Lambda Value\", fontsize=12)\n",
    "    ax.grid(linestyle=\"--\")\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_title('Lambda History', fontsize=17)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "\n",
    "    #   case\n",
    "    results.display_contour()\n",
    "    elapsed = timeit.default_timer() - t\n",
    "\n",
    "    if prob == \"Direct\":\n",
    "        return results.abs_err_u, results.rel_err_u\n",
    "    else:\n",
    "        return model.lambda_1_log[-1][0][0], elapsed\n",
    "\n",
    "\n",
    "# RUN\n",
    "if __name__ == \"__main__\": \n",
    "    layers = [2] # num of hidden layers\n",
    "    neurons = [5] # num of neurons\n",
    "    noises = [0.00, 0.01, 0.03] #, 0.15] # noise level\n",
    "    seed = [543212345, 2345, 5432123, 543212, 54321, 345, 543, 54, 5, 4] # num of hidden layers\n",
    "    # N_u = [500, 1000, 1500, 2000] # num of training data \n",
    "\n",
    "#     abs_err_table = np.zeros((len(layers), len(neurons)))\n",
    "#     rel_err_table = np.zeros((len(layers), len(neurons)))\n",
    "\n",
    "    lambda_1_table1 = np.zeros((len(noises),len(seed)))\n",
    "#     lambda_1_table2 = np.zeros(len(noises))\n",
    "    time_table = np.zeros((len(noises),len(seed)))\n",
    "    model_list = []\n",
    "\n",
    "    # for i in range(len(layers)):\n",
    "    #   for j in range(len(neurons)):\n",
    "    #       abs_err_table[i,j], rel_err_table[i,j] = main_loop(layers[i], neurons[j], noises[0], BC=\"Dirichlet\", prob=\"Direct\")\n",
    "\n",
    "    # np.savetxt('./abs_err_table.csv', abs_err_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "    # np.savetxt('./rel_err_table.csv', rel_err_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "\n",
    "    for i in range(len(noises)):\n",
    "        for j in range(0, len(seed)):\n",
    "            lambda_1_table1[i, j], time_table[i, j] =  main_loop(2, 5, noises[i], BC=\"Dirichlet\", prob=\"Inverse\", seed=seed[j])\n",
    "    #     lambda_1_table2[i] =  main_loop(6, 15, noises[i], BC=\"Dirichlet\", prob=\"Inverse\")\n",
    "    \n",
    "    \n",
    "\n",
    "    np.savetxt('./lambda_1_table1_swish.csv', lambda_1_table1, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "    np.savetxt('./time_table_swish.csv', time_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "    \n",
    "#     lambda_1_log = []\n",
    "#     for i in range(0, len(model_list)):\n",
    "#         model_list[i].save_model(f\"noise_0%_seed_{seed[i]}\")\n",
    "#         lambda_1_log.append(np.array(model_list[i].lambda_1_log)[:,0].flatten()[:,None])\n",
    "    \n",
    "#         np.savetxt(f'./lambda_1_log_noise_0%_seed_{seed[i]}.csv', lambda_1_log, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "        \n",
    "#     for i in range(len(lambda_log)):\n",
    "\n",
    "       \n",
    "\n",
    "    #   np.savetxt('./lambda_1_table2.csv', lambda_1_table2, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sigmoid\n",
    "def main_loop(layers, neurons, noise, BC, prob, seed):\n",
    "    t = timeit.default_timer()\n",
    "    print(noise)\n",
    "    params = generate_param()\n",
    "    print(seed)\n",
    "    params[\"network\"][\"layers\"] = [2] + layers*[neurons] + [1]\n",
    "    params[\"data\"][\"noise\"] = noise\n",
    "    params[\"data\"][\"seed\"] = seed\n",
    "    loc_num = 4\n",
    "    n = 101\n",
    "    params[\"data\"][\"n_wall\"] = n\n",
    "    params[\"data\"][\"loc\"] = np.linspace(0, 1, loc_num) #in %\n",
    "    params[\"data\"][\"meu\"] = [n]*loc_num\n",
    "    \n",
    "#     fun = \"relu\"\n",
    "#     fun = \"tanh\"\n",
    "#     fun = \"swish\"\n",
    "    fun = \"sigmoid\"\n",
    "    params[\"network\"][\"act_fun\"] = fun\n",
    "\n",
    "    # case = CasesChannel()\n",
    "    case = CasesChannel(add_noise=True)\n",
    "    case.generate_train_data(param=params, bc_type=BC)\n",
    "    case.generate_test_data(param=params)\n",
    "    case.plot()\n",
    "\n",
    "    model = PinnVP(data=case.data, adaptive_model=False,\n",
    "                 params=params, bc_type=BC, problem_type=prob)\n",
    "\n",
    "    model.fit_newton()\n",
    "    model_list.append(model)\n",
    "    \n",
    "\n",
    "    results = PostChannel(model=model,\n",
    "                      params=params,\n",
    "                      save_fig=False)\n",
    "\n",
    "    results.display_loss()\n",
    "    \n",
    "#     lambda_log = []\n",
    "    lambda_log = (np.array(model.lambda_1_log)[:,0])\n",
    "    x = np.linspace(1, len(lambda_log), len(lambda_log))\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6), constrained_layout=True, dpi=300)\n",
    "    ax.plot(x, lambda_log, \"r\", linestyle=\"solid\", label=\"Noise free\")\n",
    "    ax.set_xlabel(\"Iterations\", fontsize=12)\n",
    "    ax.set_ylabel(\"Lambda Value\", fontsize=12)\n",
    "    ax.grid(linestyle=\"--\")\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_title('Lambda History', fontsize=17)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "\n",
    "    #   case\n",
    "    results.display_contour()\n",
    "    elapsed = timeit.default_timer() - t\n",
    "\n",
    "    if prob == \"Direct\":\n",
    "        return results.abs_err_u, results.rel_err_u\n",
    "    else:\n",
    "        return model.lambda_1_log[-1][0][0], elapsed\n",
    "\n",
    "\n",
    "# RUN\n",
    "if __name__ == \"__main__\": \n",
    "    layers = [2] # num of hidden layers\n",
    "    neurons = [5] # num of neurons\n",
    "    noises = [0.00, 0.01, 0.03] #, 0.15] # noise level\n",
    "    seed = [543212345, 2345, 5432123, 543212, 54321, 345, 543, 54, 5, 4] # num of hidden layers\n",
    "    # N_u = [500, 1000, 1500, 2000] # num of training data \n",
    "\n",
    "#     abs_err_table = np.zeros((len(layers), len(neurons)))\n",
    "#     rel_err_table = np.zeros((len(layers), len(neurons)))\n",
    "\n",
    "    lambda_1_table1 = np.zeros((len(noises),len(seed)))\n",
    "#     lambda_1_table2 = np.zeros(len(noises))\n",
    "    time_table = np.zeros((len(noises),len(seed)))\n",
    "    model_list = []\n",
    "\n",
    "    # for i in range(len(layers)):\n",
    "    #   for j in range(len(neurons)):\n",
    "    #       abs_err_table[i,j], rel_err_table[i,j] = main_loop(layers[i], neurons[j], noises[0], BC=\"Dirichlet\", prob=\"Direct\")\n",
    "\n",
    "    # np.savetxt('./abs_err_table.csv', abs_err_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "    # np.savetxt('./rel_err_table.csv', rel_err_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "\n",
    "    for i in range(len(noises)):\n",
    "        for j in range(0, len(seed)):\n",
    "            lambda_1_table1[i, j], time_table[i, j] =  main_loop(2, 5, noises[i], BC=\"Dirichlet\", prob=\"Inverse\", seed=seed[j])\n",
    "    #     lambda_1_table2[i] =  main_loop(6, 15, noises[i], BC=\"Dirichlet\", prob=\"Inverse\")\n",
    "    \n",
    "    \n",
    "\n",
    "    np.savetxt('./lambda_1_table1_sigmoid.csv', lambda_1_table1, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "    np.savetxt('./time_table_sigmoid.csv', time_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "    \n",
    "#     lambda_1_log = []\n",
    "#     for i in range(0, len(model_list)):\n",
    "#         model_list[i].save_model(f\"noise_0%_seed_{seed[i]}\")\n",
    "#         lambda_1_log.append(np.array(model_list[i].lambda_1_log)[:,0].flatten()[:,None])\n",
    "    \n",
    "#         np.savetxt(f'./lambda_1_log_noise_0%_seed_{seed[i]}.csv', lambda_1_log, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "        \n",
    "#     for i in range(len(lambda_log)):\n",
    "\n",
    "       \n",
    "\n",
    "    #   np.savetxt('./lambda_1_table2.csv', lambda_1_table2, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop(layers, neurons, noise, BC, prob, seed):\n",
    "    t = timeit.default_timer()\n",
    "    print(noise)\n",
    "    params = generate_param()\n",
    "#     print(noise)\n",
    "    params[\"network\"][\"layers\"] = [2] + layers*[neurons] + [1]\n",
    "    params[\"data\"][\"noise\"] = noise\n",
    "    params[\"data\"][\"seed\"] = seed\n",
    "    \n",
    "    loc_num = 4\n",
    "    n = 101\n",
    "    params[\"data\"][\"n_wall\"] = n\n",
    "    params[\"data\"][\"loc\"] = np.linspace(0, 1, loc_num) #in %\n",
    "    params[\"data\"][\"meu\"] = [n]*loc_num\n",
    "    \n",
    "#     fun = \"relu\"\n",
    "    fun = \"tanh\"\n",
    "#     fun = \"swish\"\n",
    "#     fun = \"sigmoid\"\n",
    "    params[\"network\"][\"act_fun\"] = fun\n",
    "\n",
    "    # case = CasesChannel()\n",
    "    case = CasesChannel(add_noise=True)\n",
    "    case.generate_train_data(param=params, bc_type=BC)\n",
    "    case.generate_test_data(param=params)\n",
    "\n",
    "    model = PinnVP(data=case.data, adaptive_model=False,\n",
    "                 params=params, bc_type=BC, problem_type=prob)\n",
    "\n",
    "    model.fit_newton()\n",
    "    model_list.append(model)\n",
    "    \n",
    "\n",
    "    results = PostChannel(model=model,\n",
    "                      params=params,\n",
    "                      save_fig=False)\n",
    "\n",
    "    results.display_loss()\n",
    "    #   case\n",
    "    results.display_contour()\n",
    "    elapsed = timeit.default_timer() - t\n",
    "    lambda_log.append(model.lambda_1_log)\n",
    "\n",
    "    if prob == \"Direct\":\n",
    "        return results.abs_err_u, results.rel_err_u\n",
    "    else:\n",
    "        return model.lambda_1_log[-1][0][0], elapsed\n",
    "\n",
    "\n",
    "# RUN\n",
    "if __name__ == \"__main__\": \n",
    "    layers = [2] # num of hidden layers\n",
    "    neurons = [5] # num of neurons\n",
    "    noises = [0.00, 0.01, 0.03] # noise level\n",
    "    seed = [12345] #, 2345, 5432123, 543212, 54321, 345, 543, 54, 5, 4] # num of hidden layers\n",
    "    # N_u = [500, 1000, 1500, 2000] # num of training data \n",
    "\n",
    "    abs_err_table = np.zeros((len(layers), len(neurons)))\n",
    "    rel_err_table = np.zeros((len(layers), len(neurons)))\n",
    "\n",
    "    lambda_1_table1 = np.zeros((len(noises),len(seed)))\n",
    "    lambda_1_table2 = np.zeros(len(noises))\n",
    "    time_table = np.zeros((len(noises), len(seed)))\n",
    "    model_list = []\n",
    "    lambda_log = []\n",
    "\n",
    "    for i in range(len(noises)):\n",
    "        for j in range(0, len(seed)):\n",
    "            lambda_1_table1[i, j], time_table[i,j] =  main_loop(2, 5, noises[i], BC=\"Dirichlet\", prob=\"Inverse\", seed=seed[j])\n",
    "    #     lambda_1_table2[i] =  main_loop(6, 15, noises[i], BC=\"Dirichlet\", prob=\"Inverse\")\n",
    "    \n",
    "    for i in range(len(model_list)):\n",
    "        model_list[i].save_model(f'model_noise_{noises[i]}.pickle')\n",
    "        lambda_1_log = np.array(model_list[i].lambda_1_log)[:,0].flatten()[:,None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lambda_log)):\n",
    "    \n",
    "    np.savetxt(f'./lambda_1_log_noise_{noises[i]}.csv', np.array(lambda_log[i])[:,0], delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = [0.00, 0.01, 0.03]\n",
    "lambda_ = []\n",
    "for i in noise:\n",
    "    lambda_.append(pd.read_csv(f\"lambda_1_log_noise_{i}.csv\",header=None).to_numpy())\n",
    "    \n",
    "\n",
    "x0 = np.linspace(0, len(lambda_[0][:,0]), len(lambda_[0][:,0]))\n",
    "x1 = np.linspace(0, len(lambda_[1][:,0]), len(lambda_[1][:,0]))\n",
    "x2 = np.linspace(0, len(lambda_[2][:,0]), len(lambda_[2][:,0]))\n",
    "# x3 = np.linspace(0, len(lambda_[3][:,0]), len(lambda_[3][:,0]))\n",
    "\n",
    "lambda_new = []\n",
    "for i in range(0,len(noise)):\n",
    "    lambda_edited = np.zeros((len(lambda_[i]),1))\n",
    "    for j in range(0, len(lambda_[i])):\n",
    "        lambda_edited[j,0] = lambda_[i][j][0].split(\" \")[0]\n",
    "    lambda_new.append(lambda_edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# base = pd.read_csv(\"baseline_test.csv\").to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 8), constrained_layout=True, dpi=300)\n",
    "# ax.plot(base[:,0], base[:,1], \"k\", linestyle=\"solid\", label=\"Baseline\")\n",
    "ax.plot(x0, lambda_new[0][:,0], \"r\", linestyle=\"solid\", label=\"Noise free\")\n",
    "ax.plot(x1, lambda_new[1][:,0], \"b\", linestyle=\"dashed\", label=\"1\\% noise\")\n",
    "ax.plot(x2, lambda_new[2][:,0], \"g\", linestyle=\"dashdot\", label=\"3\\% noise\")\n",
    "# ax.plot(x3, lambda_[3][:,0], \"c\", linestyle=\"dotted\", label=\"15\\% noise\")\n",
    "# ax.plot(M[4][:,0], M[4][:,1], \"purple\", linestyle=\"-.\", label=\"M = 10000\")\n",
    "\n",
    "# ax.set_xlim(0, x1[-1])\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Iterations\", fontsize=30)\n",
    "ax.set_ylabel(\"$\\lambda$\", fontsize=30)\n",
    "ax.grid(linestyle=\"--\")\n",
    "ax.legend(fontsize=25)\n",
    "ax.set_title('$\\lambda$ History', fontsize=40)\n",
    "ax.tick_params(axis='both', which='major', labelsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_new = []\n",
    "for i in range(0,len(noise)):\n",
    "    lambda_edited = np.zeros((len(lambda_[i]),1))\n",
    "    for j in range(0, len(lambda_[i])):\n",
    "        lambda_edited[j,0] = lambda_[i][j][0].split(\" \")[0]\n",
    "    lambda_new.append(lambda_edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(lambda_[0][5][0].split(\" \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lambda_new)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base = pd.read_csv(\"baseline_test.csv\").to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 8), constrained_layout=True, dpi=300)\n",
    "# ax.plot(base[:,0], base[:,1], \"k\", linestyle=\"solid\", label=\"Baseline\")\n",
    "ax.plot(x0, lambda_new[0][:,0], \"r\", linestyle=\"solid\", label=\"Noise free\")\n",
    "ax.plot(x1, lambda_new[1][:,0], \"b\", linestyle=\"dashed\", label=\"1\\% noise\")\n",
    "ax.plot(x2, lambda_new[2][:,0], \"g\", linestyle=\"dashdot\", label=\"3\\% noise\")\n",
    "# ax.plot(x3, lambda_[3][:,0], \"c\", linestyle=\"dotted\", label=\"15\\% noise\")\n",
    "# ax.plot(M[4][:,0], M[4][:,1], \"purple\", linestyle=\"-.\", label=\"M = 10000\")\n",
    "\n",
    "# ax.set_xlim(0, x1[-1])\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Iterations\", fontsize=30)\n",
    "ax.set_ylabel(\"$\\lambda$\", fontsize=30)\n",
    "ax.grid(linestyle=\"--\")\n",
    "ax.legend(fontsize=25)\n",
    "ax.set_title('$\\lambda$ History', fontsize=40)\n",
    "ax.tick_params(axis='both', which='major', labelsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 51 n\n",
    "def main_loop(layers, neurons, noise, BC, prob, seed):\n",
    "    t = timeit.default_timer()\n",
    "    print(noise)\n",
    "    params = generate_param()\n",
    "    print(seed)\n",
    "    params[\"network\"][\"layers\"] = [2] + layers*[neurons] + [1]\n",
    "    params[\"data\"][\"noise\"] = noise\n",
    "    params[\"data\"][\"seed\"] = seed\n",
    "    loc_num = 4\n",
    "    n = 51\n",
    "    params[\"data\"][\"n_wall\"] = n\n",
    "    params[\"data\"][\"loc\"] = np.linspace(0, 1, loc_num) #in %\n",
    "    params[\"data\"][\"meu\"] = [n]*loc_num\n",
    "\n",
    "    # case = CasesChannel()\n",
    "    case = CasesChannel(add_noise=True)\n",
    "    case.generate_train_data(param=params, bc_type=BC)\n",
    "    case.generate_test_data(param=params)\n",
    "    case.plot()\n",
    "\n",
    "    model = PinnVP(data=case.data, adaptive_model=False,\n",
    "                 params=params, bc_type=BC, problem_type=prob)\n",
    "\n",
    "    model.fit_newton()\n",
    "    model_list.append(model)\n",
    "    \n",
    "\n",
    "    results = PostChannel(model=model,\n",
    "                      params=params,\n",
    "                      save_fig=False)\n",
    "\n",
    "    results.display_loss()\n",
    "    \n",
    "#     lambda_log = []\n",
    "    lambda_log = (np.array(model.lambda_1_log)[:,0])\n",
    "    x = np.linspace(1, len(lambda_log), len(lambda_log))\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6), constrained_layout=True, dpi=300)\n",
    "    ax.plot(x, lambda_log, \"r\", linestyle=\"solid\", label=\"Noise free\")\n",
    "    ax.set_xlabel(\"Iterations\", fontsize=12)\n",
    "    ax.set_ylabel(\"Lambda Value\", fontsize=12)\n",
    "    ax.grid(linestyle=\"--\")\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_title('Lambda History', fontsize=17)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "\n",
    "    #   case\n",
    "    results.display_contour()\n",
    "    elapsed = timeit.default_timer() - t\n",
    "\n",
    "    if prob == \"Direct\":\n",
    "        return results.abs_err_u, results.rel_err_u\n",
    "    else:\n",
    "        return model.lambda_1_log[-1][0][0], elapsed\n",
    "\n",
    "\n",
    "# RUN\n",
    "if __name__ == \"__main__\": \n",
    "    layers = [2] # num of hidden layers\n",
    "    neurons = [5] # num of neurons\n",
    "    noises = [0.00, 0.01, 0.03] #, 0.15] # noise level\n",
    "    seed = [543212345, 2345, 5432123, 543212, 54321, 345, 543, 54, 5, 4] # num of hidden layers\n",
    "    # N_u = [500, 1000, 1500, 2000] # num of training data \n",
    "\n",
    "#     abs_err_table = np.zeros((len(layers), len(neurons)))\n",
    "#     rel_err_table = np.zeros((len(layers), len(neurons)))\n",
    "\n",
    "    lambda_1_table1 = np.zeros((len(noises),len(seed)))\n",
    "#     lambda_1_table2 = np.zeros(len(noises))\n",
    "    time_table = np.zeros((len(noises),len(seed)))\n",
    "    model_list = []\n",
    "\n",
    "    # for i in range(len(layers)):\n",
    "    #   for j in range(len(neurons)):\n",
    "    #       abs_err_table[i,j], rel_err_table[i,j] = main_loop(layers[i], neurons[j], noises[0], BC=\"Dirichlet\", prob=\"Direct\")\n",
    "\n",
    "    # np.savetxt('./abs_err_table.csv', abs_err_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "    # np.savetxt('./rel_err_table.csv', rel_err_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "\n",
    "    for i in range(len(noises)):\n",
    "        for j in range(0, len(seed)):\n",
    "            lambda_1_table1[i, j], time_table[i, j] =  main_loop(2, 5, noises[i], BC=\"Dirichlet\", prob=\"Inverse\", seed=seed[j])\n",
    "    #     lambda_1_table2[i] =  main_loop(6, 15, noises[i], BC=\"Dirichlet\", prob=\"Inverse\")\n",
    "    \n",
    "    \n",
    "\n",
    "    np.savetxt('./lambda_1_table1_51_point.csv', lambda_1_table1, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "    np.savetxt('./time_table_51_point.csv', time_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "    \n",
    "#     lambda_1_log = []\n",
    "#     for i in range(0, len(model_list)):\n",
    "#         model_list[i].save_model(f\"noise_0%_seed_{seed[i]}\")\n",
    "#         lambda_1_log.append(np.array(model_list[i].lambda_1_log)[:,0].flatten()[:,None])\n",
    "    \n",
    "#         np.savetxt(f'./lambda_1_log_noise_0%_seed_{seed[i]}.csv', lambda_1_log, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "        \n",
    "#     for i in range(len(lambda_log)):\n",
    "\n",
    "       \n",
    "\n",
    "    #   np.savetxt('./lambda_1_table2.csv', lambda_1_table2, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 201 n\n",
    "def main_loop(layers, neurons, noise, BC, prob, seed):\n",
    "    t = timeit.default_timer()\n",
    "    print(noise)\n",
    "    params = generate_param()\n",
    "    print(seed)\n",
    "    params[\"network\"][\"layers\"] = [2] + layers*[neurons] + [1]\n",
    "    params[\"data\"][\"noise\"] = noise\n",
    "    params[\"data\"][\"seed\"] = seed\n",
    "    loc_num = 4\n",
    "    n = 201\n",
    "    params[\"data\"][\"n_wall\"] = n\n",
    "    params[\"data\"][\"loc\"] = np.linspace(0, 1, loc_num) #in %\n",
    "    params[\"data\"][\"meu\"] = [n]*loc_num\n",
    "\n",
    "    # case = CasesChannel()\n",
    "    case = CasesChannel(add_noise=True)\n",
    "    case.generate_train_data(param=params, bc_type=BC)\n",
    "    case.generate_test_data(param=params)\n",
    "    case.plot()\n",
    "\n",
    "    model = PinnVP(data=case.data, adaptive_model=False,\n",
    "                 params=params, bc_type=BC, problem_type=prob)\n",
    "\n",
    "    model.fit_newton()\n",
    "    model_list.append(model)\n",
    "    \n",
    "\n",
    "    results = PostChannel(model=model,\n",
    "                      params=params,\n",
    "                      save_fig=False)\n",
    "\n",
    "    results.display_loss()\n",
    "    \n",
    "#     lambda_log = []\n",
    "    lambda_log = (np.array(model.lambda_1_log)[:,0])\n",
    "    x = np.linspace(1, len(lambda_log), len(lambda_log))\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 6), constrained_layout=True, dpi=300)\n",
    "    ax.plot(x, lambda_log, \"r\", linestyle=\"solid\", label=\"Noise free\")\n",
    "    ax.set_xlabel(\"Iterations\", fontsize=12)\n",
    "    ax.set_ylabel(\"Lambda Value\", fontsize=12)\n",
    "    ax.grid(linestyle=\"--\")\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_title('Lambda History', fontsize=17)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "\n",
    "    #   case\n",
    "    results.display_contour()\n",
    "    elapsed = timeit.default_timer() - t\n",
    "\n",
    "    if prob == \"Direct\":\n",
    "        return results.abs_err_u, results.rel_err_u\n",
    "    else:\n",
    "        return model.lambda_1_log[-1][0][0], elapsed\n",
    "\n",
    "\n",
    "# RUN\n",
    "if __name__ == \"__main__\": \n",
    "    layers = [2] # num of hidden layers\n",
    "    neurons = [5] # num of neurons\n",
    "    noises = [0.00, 0.01, 0.03] #, 0.15] # noise level\n",
    "    seed = [543212345, 2345, 5432123, 543212, 54321, 345, 543, 54, 5, 4] # num of hidden layers\n",
    "    # N_u = [500, 1000, 1500, 2000] # num of training data \n",
    "\n",
    "#     abs_err_table = np.zeros((len(layers), len(neurons)))\n",
    "#     rel_err_table = np.zeros((len(layers), len(neurons)))\n",
    "\n",
    "    lambda_1_table1 = np.zeros((len(noises),len(seed)))\n",
    "#     lambda_1_table2 = np.zeros(len(noises))\n",
    "    time_table = np.zeros((len(noises),len(seed)))\n",
    "    model_list = []\n",
    "\n",
    "    # for i in range(len(layers)):\n",
    "    #   for j in range(len(neurons)):\n",
    "    #       abs_err_table[i,j], rel_err_table[i,j] = main_loop(layers[i], neurons[j], noises[0], BC=\"Dirichlet\", prob=\"Direct\")\n",
    "\n",
    "    # np.savetxt('./abs_err_table.csv', abs_err_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "    # np.savetxt('./rel_err_table.csv', rel_err_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "\n",
    "    for i in range(len(noises)):\n",
    "        for j in range(0, len(seed)):\n",
    "            lambda_1_table1[i, j], time_table[i, j] =  main_loop(2, 5, noises[i], BC=\"Dirichlet\", prob=\"Inverse\", seed=seed[j])\n",
    "    #     lambda_1_table2[i] =  main_loop(6, 15, noises[i], BC=\"Dirichlet\", prob=\"Inverse\")\n",
    "    \n",
    "    \n",
    "\n",
    "    np.savetxt('./lambda_1_table1_201_point.csv', lambda_1_table1, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "    np.savetxt('./time_table_201_point.csv', time_table, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "    \n",
    "#     lambda_1_log = []\n",
    "#     for i in range(0, len(model_list)):\n",
    "#         model_list[i].save_model(f\"noise_0%_seed_{seed[i]}\")\n",
    "#         lambda_1_log.append(np.array(model_list[i].lambda_1_log)[:,0].flatten()[:,None])\n",
    "    \n",
    "#         np.savetxt(f'./lambda_1_log_noise_0%_seed_{seed[i]}.csv', lambda_1_log, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "        \n",
    "#     for i in range(len(lambda_log)):\n",
    "\n",
    "       \n",
    "\n",
    "    #   np.savetxt('./lambda_1_table2.csv', lambda_1_table2, delimiter=' ', fmt='%2.6f', newline=' \\\\\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = noises\n",
    "lambda_ = []\n",
    "for i in noise:\n",
    "    lambda_.append(pd.read_csv(f\"lambda_1_log_noise_{i}.csv\").to_numpy())\n",
    "    \n",
    "\n",
    "x0 = np.linspace(0, len(lambda_[0][:,0]), len(lambda_[0][:,0]))\n",
    "x1 = np.linspace(0, len(lambda_[1][:,0]), len(lambda_[1][:,0]))\n",
    "x2 = np.linspace(0, len(lambda_[2][:,0]), len(lambda_[2][:,0]))\n",
    "x3 = np.linspace(0, len(lambda_[3][:,0]), len(lambda_[3][:,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base = pd.read_csv(\"baseline_test.csv\").to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 8), constrained_layout=True, dpi=300)\n",
    "# ax.plot(base[:,0], base[:,1], \"k\", linestyle=\"solid\", label=\"Baseline\")\n",
    "ax.plot(x0, lambda_[0][:,0], \"r\", linestyle=\"solid\", label=\"Noise free\")\n",
    "ax.plot(x1, lambda_[1][:,0], \"b\", linestyle=\"dashed\", label=\"5 percent noise\")\n",
    "ax.plot(x2, lambda_[2][:,0], \"g\", linestyle=\"dashdot\", label=\"10 percent noise\")\n",
    "ax.plot(x3, lambda_[3][:,0], \"c\", linestyle=\"dotted\", label=\"15 percent noise\")\n",
    "# ax.plot(M[4][:,0], M[4][:,1], \"purple\", linestyle=\"-.\", label=\"M = 10000\")\n",
    "\n",
    "# ax.set_xlim(0, x1[-1])\n",
    "# ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Iterations\", fontsize=24)\n",
    "ax.set_ylabel(\"$\\lambda$\", fontsize=24)\n",
    "ax.grid(linestyle=\"--\")\n",
    "ax.legend(fontsize=20)\n",
    "ax.set_title('$\\lambda$ History', fontsize=35)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Heat Conduction - Inverse - 1 Parameter.ipynb",
   "provenance": [
    {
     "file_id": "1DquI6-opM6DDVTGby5XDaQdDuZQRlcl1",
     "timestamp": 1643096956064
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
